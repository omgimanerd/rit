\documentclass[letterpaper, 12pt]{math}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linguistics]{forest}

\title{Probability and Statistics}
\author{Alvin Lin}
\date{Probability and Statistics: January 2017 - May 2017}

\begin{document}

\maketitle

\section*{Conditional Probability}
Let \( A \) and \( B \) be events of an experiment. The conditional probability
of \( A \) given that \( B \) has occurred is:
\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]

\subsection*{Idea Behind This Formula}
Roll a die:
\begin{align*}
  A&: getting\ a\ 2 \\
  B&: getting\ an\ even\ number \\
\end{align*}
\[ P(A) = \frac{\#\ of\ outcomes\ favorable\ to\ A}{total\ \#\ of\ outcomes} \]
(Under the assumption that the outcomes are equally likely) \\

Let's consider the probability of \( A \) given that \( B \) has occurred.
Suppose the dice have detectors on the faces 2, 4, and 6. After we roll the
dice, we can detect whether the dice has landed on a 2, 4, or 6, but we will
not know which of the even numbers it has landed on.
\[ P(A|B) = \frac{\#\ of\ outcomes\ favorable\ to\ A\ and\ in\ B}
   {total\ \#\ of\ outcomes\ in\ B} \]
\[ P(A|B) = \frac{1}{3} \]
Considering the situation in the previous example, find \( P(B|A) \).
\[ P(B|A) = \frac{1}{1} = 1 \]
If \( P(B) > 0 \), the conditional probability of \( A \) given that \( B \)
has occurred is \( P(A|B) = \frac{P(A \cap B)}{P(B)} \). Thus it follows
that:
\[ P(A \cap B) \times P(B) = P(A|B) \]
This holds true when \( P(B) = 0 \) as well.

\subsection*{Bayes' Theorem}
Let \( A_{1}, A_{2}, A_{3}, \dots, A_{k} \) be events of an experiment.
The events \( A_{1}, A_{2}, A_{3}, \dots, A_{k} \) are \textbf{mutually
exclusive} or \textbf{disjoint} if and only if \( A_{1} \cap A_{j} =
\varnothing \) for any \( i, j \in \{1, 2, 3, \dots, k \} \) with \( i \neq j \).
If \( A_{1}, A_{2}, A_{3}, \dots, A_{k} \) are mutually exclusive, then
\( A_{1} \cap A_{2} \cap A_{3} = \varnothing \), \( A_{1} \cap A_{2} \cap A_{4}
\cap A_{6} = \varnothing \), and so on. \par
The events \( A_{1}, A_{2}, A_{3}, \dots, A_{k} \) are \textbf{exhaustive},
if and only if \( A_{1} \cup A_{2} \cup A_{3} \cup \dots \cup A_{j} = S \).
If \( A_{1} \) and \( A_{2} \) are exhaustive, then:
\begin{align*}
  P(A_{1})+P(A_{2}) &= P(A_{1} \cup A_{2})+P(A_{1} \cap A_{2}) \\
  &(Inclusion\ Exclusion\ Principle) \\
  &= P(S)+P(A_{1} \cap A_{2}) \\
  &(since\ A_{1}\ and\ A_{2}\ are\ exhaustive) \\
  &= 1+P(A_{1} \cap A_{2}) \\
  &\geq 1
\end{align*}
Let's consider the contrapositive of the statement.
\[ If\ P(A_{1})+P(A_{2}) < 1,\ then\ A_{1}\ and\ A_{2}\ are\ not\ exhaustive. \]

\subsection*{The Law of Total Probability}
Let \( A_{1}, A_{2}, \dots, A_{k} \) be mutually exclusive and exhaustive
events. Then for any other event \( B \):
\begin{align*}
  P(B) &= P(B|A_{1})P(A_{1})+\dots+P(B|A_{k})P(A_{j}) \\
  &= \sum_{i=1}^{k}P(B|A_{i})P(A_{i})
\end{align*}

\subsection*{Example}
Roll a die:
\begin{align*}
  B&: a\ 2\ or\ a\ 3 \\
  A_{1}&: a\ number\ less\ than\ 3 \\
  A_{2}&: a\ number\ greater\ than\ 2 \\
\end{align*}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    & \( B \) & \( A_{1} \) & \( A_{2} \) & \( P(B|A_{1}) \) &
      \( P(B|A_{2}) \) \\ \hline
    1 & & \checkmark & & & \\ \hline
    2 & \checkmark & \checkmark & & \checkmark & \\ \hline
    3 & \checkmark & & \checkmark & & \checkmark \\ \hline
    4 & & & \checkmark & & \\ \hline
    5 & & & \checkmark & & \\ \hline
    6 & & & \checkmark & & \\ \hline
  \end{tabular}
\end{center}
\begin{align*}
  P(B) &= \frac{2}{6} \\
  P(B|A_{1})P(A_{1})+P(B|A_{2})P(A_{2}) &=
    (\frac{1}{2})(\frac{2}{6})+(\frac{1}{4})(\frac{4}{6}) \\
  &= \frac{1}{6}+\frac{1}{6} \\
  &= \frac{2}{6}
\end{align*}
Consistency Check:
\[ A_{1} \cap A_{2} = \varnothing \]
\[ A_{1} \cup A_{2} = S \]
\( A_{1} \) and \( A_{2} \) are mutually exclusive and exhaustive, so the law
applies.

\subsection*{Example}
An individual has 3 different email accounts. Most of her messages, in fact
70\%, come into account \#1, whereas 20\% come into account \#2, and the
remaining 10\% into account \#3. Of the messages into account \#1, only 1\% are
spam, whereas the corresponding percentages for accounts \#2 and \#3 are 2\%
and 5\%, respectively. What is the probability that a randomly selected message
is spam?
\[ P(spam) = (0.7)(0.01)+(0.2)(0.02)+(0.1)(0.05) \]
Is this an application of the Law of Total Probability? Let:
\begin{align*}
  B&: an\ email\ is\ spam \\
  A_{1}&: an\ email\ goes\ to\ account\ \#1 \\
  A_{2}&: an\ email\ goes\ to\ account\ \#2 \\
  A_{3}&: an\ email\ goes\ to\ account\ \#3 \\
\end{align*}
\[ A_{1} \cap A_{2} = \varnothing \quad A_{1} \cap A_{3} = \varnothing \quad
   A_{2} \cap A_{3} = \varnothing \]
The events \( A_{1}, A_{2}, A_{3} \) are mutually exclusive.
\[ A_{1} \cup A_{2} \cup A_{3} = S \]
The events \( A_{1}, A_{2}, A_{3} \) are exhaustive. So by the Law of Total
Probability:
\begin{align*}
  P(B) &= P(B|A_{1})P(A_{1})+P(B|A_{2})P(A_{2})+P(B|A_{3})P(A_{3}) \\
  &= (0.01)(0.7)+(0.02)(0.2)+(0.05)(0.1)
\end{align*}

\subsubsection*{Another visualization}
Fill a table with a convenient hypothetical total number of emails.
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    & account \#1 & account \#2 & account \#3 & total \\ \hline
    spam & \( 1 \times 70 \) & \( 2 \times 20 \) & \( 5 \times 10 \) & \\ \hline
    not spam & \( 99 \times 70 \) & \( 98 \times 20 \) & \( 95 \times 10 \) & \\
      \hline
    total & \( 100 \times 70 \) & \( 100 \times 20 \) & \( 100 \times 10 \) &
      \( 100 \times 100 \) \\ \hline
  \end{tabular}
\end{center}
\[ P(B) = \frac{(1)(70)+(2)(20)+(5)(10)}{(100)(100)} \]
We can also compute the conditional probabilities and check the consistency
with the Law of Total Probability.

\subsection*{Bayes' Theorem (again)}
Let \( A_{1}, A_{2}, A_{3}, \dots, A_{k} \) be a collection of \( k \)
mutually exclusive and exhaustive events with prior probabilities
\( P(A_{i})(i \in \{1, 2, \dots, k\}) \). Then for any other event B for which
\( P(B) > 0 \), the posterior/conditional probability of \( A_{j} \) given
that \( B \) has occurred is:
\[ P(A_{j}|B) = \frac{P(A_{j} \cap B)}{P(B)} =
   \frac{P(B|A_{j})P(A_{j})}{\sum_{i=0}^{k}P(B|A_{i})P(A_{i})} \]
\[ j \in \{1, 2, 3, \dots, k \} \]

\subsubsection*{Derivation}
\begin{align*}
  P(A_{j}|B) &= \frac{P(A_{j} \cap B)}{P(B)} \\
  & P(B) > 0 \\
  &= \frac{P(B|A_{j})P(A_{j})}{P(B)} \\
  & Multiplication\ Rule \\
  &= \frac{P(B|A_{j})P(A_{j})}{\sum_{i=1}^{k}P(B|A_{j})P(A_{i})} \\
  & The\ Law\ of\ Total\ Probability
\end{align*}

\subsection*{Example}
A recent Maryland highway safety study found that in 77\% of all accidents the
driver was wearing a seatbelt. Accident reports indicated that 92\% of those
drivers escaped serious injury (defined as hospitalization or death), but only
63\% of the non-belted drivers were so fortunate. What is the probability that
a driver who was seriously injured wasn't wearing a seatbelt? \par
Show the steps of finding the answer by defining \( B_{1}, B_{2} (= not\ B_{1}),
A_{1},\ and\ A_{2} \) and then using Bayes' Theorem:
\[ P(A_{2}|B_{1}) = \frac{P(B_{1}|A_{2})P(A_{2})}
   {\sum_{i=1}^{2}P(B_{1}|A_{i})P(A_{i})} \]
The probability that a driver who was seriously injured wasn't wearing a
seatbelt is equal to the probability that a driver wasn't wearing a seatbelt
given that he/she was seriously injured.
\begin{align*}
  B_{1}&: a\ driver\ was\ seriously\ injured \\
  B_{2}&: a\ driver\ was\ not\ seriously\ injured \\
  A_{1}&: a\ driver\ was\ wearing\ a\ seat\ belt \\
  A_{2}&: a\ driver\ was\ not\ wearing\ a\ seat\ belt
\end{align*}
The probability that we want to find is: \( P(A_{2}|B_{1}) \). \( A_{1} \) and
\( A_{2} \) are mutually exclusive and exhaustive.
\begin{align*}
  P(A_{1}) &= 0.77 \\
  P(A_{2}) &= 0.23 \\
  P(B_{2}|A_{1}) &= 0.92 \\
  P(B_{1}|A_{1}) &= 0.08 \\
  P(B_{2}|A_{2}) &= 0.63 \\
  P(B_{1}|A_{2}) &= 0.37 \\
  P(A_{2}|B_{1}) &= \frac{P(B_{1}|A_{2})P(A_{2})}
    {P(B_{1}|A_{1})P(A_{1})+P(B_{1}A_{2})P(A_{2})} \\
  &= \frac{0.37\times 0.23}{0.08\times 0.77+0.37\times 0.23} \\
  &\approx 0.58
\end{align*}
Show the steps of finding \( P(A_{2}|B_{1}) \) using a tree diagram.
\begin{center}
  \begin{forest}
    [
      [\( A_{1} (0.77) \)
        [\( B_{1} (0.08) \)]
        [\( B_{2} (0.92) \)]
      ]
      [\( A_{2}(0.23) \)
        [\( B_{1} (0.37) \)]
        [\( B_{2} (0.63) \)]
      ]
    ]
  \end{forest}
\end{center}
\[ P(A_{2}|B_{1}) = \frac{0.23 \times 0.37}{0.77 \times 0.08+0.23\times 0.37}
   \approx 0.58 \]

\subsection*{Example}
1 in 1000 adults is afflicted with a rare disease. Consider a test method. How
exact is the test? If a person has the disease, there is a 99\% chance they will
test positive. If a person does not ahve the disease, there is a 2\% chance they
will test positive. If a randomly selected individual is tested and the result
is positive, what is the probability that the individual has the disease? \par
By Bayes' Theorem, the probability is 4.7\%.

\end{document}

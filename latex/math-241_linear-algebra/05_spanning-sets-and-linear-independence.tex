\documentclass{math}

\title{Linear Algebra}
\author{Alvin Lin}
\date{August 2017 - December 2017}

\begin{document}

\maketitle

\section*{Spanning Sets and Linear Independence}
We want to know when one vector is a \textbf{linear combination} of some other
vectors.
\[ c_1\vec{v_1}+c_2\vec{v_2}+c_3\vec{v_3} \]
For example, is the vector \( \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix} \) a
linear combination of \( \vec{v} = \begin{bmatrix}1 \\ 0 \\ 3\end{bmatrix} \)
and \( \vec{w} = \begin{bmatrix}-1 \\ 1 \\ -3\end{bmatrix} \). The solution
would be to find scalars \( c_1,c_2,c_3 \) such that:
\[ \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix} =
  c_1\begin{bmatrix}1 \\ 0 \\ 3\end{bmatrix}+
  c_2\begin{bmatrix}-1 \\ 1 \\ -3\end{bmatrix} \]
We can treat this as a system of linear equations and solve an augmented
matrix:
\[ A' = \left[\begin{array}{cc|c}
  1 & -1 & 1 \\
  0 & 1 & 2 \\
  3 & -3 & 3
\end{array}\right] \]
Bring this to reduced row echelon form yields:
\[ A' = \left[\begin{array}{cc|c}
  1 & 0 & 3 \\
  0 & 1 & 2 \\
  0 & 0 & 0
\end{array}\right] \]
Thus, \( \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix} \) is a linear combination of
\( \vec{v},\vec{w} \) through the constants:
\[ \begin{bmatrix}c_1 \\ c_2\end{bmatrix} =
  \begin{bmatrix}3 \\ 2\end{bmatrix} \]

\subsection*{Describing a Linear System}
There are three equivalent ways to specify a linear system:
\begin{enumerate}
  \item \[ \begin{cases}
    a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n &= b_1 \\
    \vdots & \\
    a_{m1}x_1+a_{m2}x_2+\dots+a_{mn}x_n &= b_m
  \end{cases} \]
  \item Let \( \vec{A_i} \) be a column vector with entries \( a_i \):
    \[ x_1\vec{A_1}+x_2\vec{A_2}+\dots+x_n\vec{A_n} = \vec{b} \]
  \item \( A\vec{x} = \vec{b} \) where \( A \) is an \( m\times x \) matrix:
    \[ \vec{x} = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix} \]
    \[ \vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ \vdots \\ b_m\end{bmatrix} \]
    where we multiply \( A\vec{x} = \vec{b} \) using:
    \[ b_i = \text{ith row of A}\times\vec{x} \]
\end{enumerate}
\textbf{Theorem:} A system of linear equations with augmented matrix
\( [A|\vec{b}] \) is consistent if and only if \( \vec{b} \) is a linear
combination of the columns of \( A \).

\subsection*{Spanning Sets}
Let \( \{\vec{v_1},\dots,\vec{v_k}\} \) be a set of vectors.
\[ Span(\{\vec{v_1},\dots,\vec{v_k}\}) = \left\{\sum_{i=1}^{k}c_i\vec{v_i}\mid
  c_i \text{'s are scalars}\right\} \]
The span of:
\[ \vec{e_1} = \begin{bmatrix}1 \\ 0\end{bmatrix}\quad
  \vec{e_2} = \begin{bmatrix}0 \\ 1\end{bmatrix} \]
is equal to:
\begin{align*}
  Span(\{\vec{e_1},\vec{e_2}\}) &=
    \{c_1\vec{e_1}+c_2\vec{e_2}\mid c_1,c_2\in\R\} \\
  &= \left\{\begin{bmatrix}c_1 \\ 0\end{bmatrix}+
    \begin{bmatrix}0 \\ c_2\end{bmatrix}\mid c_1,c_2\in\R\right\} \\
  &= \left\{\begin{bmatrix}c_1 \\ c_2\end{bmatrix}\mid c_2,c_2\in\R\right\} \\
  &= \R^2
\end{align*}
Is a vector \( \vec{v} \) in \( Span(\{\vec{u},\vec{w}\}) \)? Can we find
scalars \( c_1,c_2 \) such that \( \vec{v} = c_1\vec{u}+c_2\vec{w} \)? This
translates to the following:
\[ A = \begin{bmatrix}\vec{u} & \vec{w}\end{bmatrix}\quad
  \vec{x} = \begin{bmatrix}c_1 \\ c_2\end{bmatrix} \]
And we are essentially solving a system of linear equations:
\[ A' = [A\mid\vec{v}] \]

\subsection*{Linear Independence}
We say vectors \( \vec{v_1},\vec{v_2},\dots,\vec{v_m} \) are \textbf{linearly
independent} if:
\[ \sum_{i=1}^{n}c_i\vec{v_i} = \vec{0} \Rightarrow \forall{i}(c_i = 0) \]
In terms of matrices, let \( A = \begin{bmatrix}\vec{v_1} & \vec{v_2} & \dots
\vec{v_m}\end{bmatrix} \). Examining \( A\vec{x} = \vec{0} \), the only
solution for \( \vec{x} \) is:
\[ \vec{x} = \begin{bmatrix}c_1 \\ c_2 \\ \dots \\ c_m\end{bmatrix} =
  \begin{bmatrix} 0 \\ 0 \\ \dots \\ 0\end{bmatrix} \]
A set of vectors \( \vec{v_1},\vec{v_2},\dots,\vec{v_m} \) is \textbf{linearly
dependent} if it is not linearly independent.

\subsubsection*{Example}
Are the vectors \( \vec{0},\vec{v_1},\vec{v_2},\dots,\vec{v_m} \) linearly
dependent or indepedent?
\[ c_1(\vec{0})+c_2\vec{v_1}+c_3\vec{v_2}+\dots+c_{m+1}\vec{v_m} = \vec{0} \]
\( c_1 \) can be 1, so this set of vectors is linearly dependent.

\subsubsection*{Example}
Are the vectors
\[ \vec{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}\quad
  \vec{w} = \begin{bmatrix}1 \\ 2\end{bmatrix} \]
linearly independent or linearly dependent?
\[ c_1\vec{v}+c_2\vec{w} \stackrel{?}{=} \vec{0} \]
\[ \begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}
  \begin{bmatrix}c_1 \\ c_2\end{bmatrix} =
  \begin{bmatrix}0 \\ 0\end{bmatrix} \]
This has a unique solution, so \( \vec{v},\vec{w} \) are linearly independent.

\subsubsection*{Example}
The general equation of the plane contains points (1,0,3), (-1,1,-3), and the
origin. It has equation \( ax+by+cz = 0 \). Find \( a,b,c \).
\begin{align*}
  a(1)+3c &= 0 \\
  a(-1)+1b-3c &= 0 \\
  \left[\begin{array}{ccc|c}
    1 & 0 & 3 & 0 \\
    -1 & 1 & -3 & 0 \\
  \end{array}\right] &= \left[\begin{array}{ccc|c}
    1 & 0 & 3 & 0 \\
    0 & 1 & 0 & 0
  \end{array}\right] \\
  a+3c &= 0 \\
  a &= -3c \\
  b &= 0 \\
  -3cx+cz &= 0 \\
  -3x+z &= 0
\end{align*}

\subsubsection*{Example}
Prove that \( \vec{u},\vec{v},\vec{w}\in span(\vec{u},\vec{v},\vec{w}) \).
\begin{align*}
  \vec{u} &= 1\vec{u}+0\vec{v}+0\vec{w} \\
  \vec{v} &= 0\vec{u}+1\vec{v}+0\vec{w} \\
  \vec{w} &= 0\vec{u}+0\vec{v}+1\vec{w} \\
  \vec{u},\vec{v},\vec{w} &\in span(\vec{u},\vec{v},\vec{w})
\end{align*}

\subsubsection*{Example}
Prove that \( \vec{u},\vec{v},\vec{w}\in span(\vec{u},\vec{u}+\vec{v},\vec{u}+
\vec{v}+\vec{w}) \).
\begin{align*}
  \vec{u} &= 1\vec{u}+0(\vec{u}+\vec{v})+0(\vec{u}+\vec{v}+\vec{w}) \\
  \vec{v} &= -1\vec{u}+1(\vec{u}+\vec{v})+0(\vec{u}+\vec{v}+\vec{w}) \\
  \vec{w} &= 0\vec{u}-1(\vec{u}+\vec{v})+1(\vec{u}+\vec{v}+\vec{w}) \\
  \vec{u},\vec{v},\vec{w} &\in
    span(\vec{u},\vec{u}+\vec{v},\vec{u}+\vec{v}+\vec{w})
\end{align*}

\subsection*{Useful Fact}
Suppose we have \( m \) vectors in \( \R^n \), where \( m > n \). Those vectors
are linearly dependent.
\[ A = \begin{bmatrix}\vec{v_1} & \vec{v_2} & \dots & \vec{v_m}\end{bmatrix} \]
Examine: \( A\vec{x} = \vec{0} \). As a consequence of the rank theorem, which
states that the number of free variables is equal to the number of columns of
\( A \) minus the rank of A, \( A\vec{x} = \vec{0} \) has a non-trivial
solution, hence the columns of \( A \) are linearly dependent.

\subsubsection*{Example}
Prove that if \( \vec{u_1},\dots,\vec{u_m}\in\R^n \) where \( S = \{\vec{u_1},
\dots,\vec{u_k}\} \) and \( T = \{\vec{u_1},\dots,\vec{u_k},\vec{u_{k+1}},\dots,
\vec{u_m}\} \) that \( span(S)\subseteq span(T) \).
Let \( \alpha\in span(S) \):
\begin{align*}
  \alpha &= c_1\vec{u_1}+c_2\vec{u_2}+\dots+c_k\vec{u_k} \\
  &= c_1\vec{u_1}+\dots+c_k\vec{u_k}+0\vec{u_{k+1}}+0\vec{u_{k+2}}+0\vec{u_m} \\
  &\in span(T)
\end{align*}
Deduce if \( \R^n = span(S) \), then \( \R^n = span(T) \).
\[ \R^n = span(S) \subseteq span(T) \subseteq \R^n \]
\[ span(T) = \R^n \]

\subsubsection*{Example}
Suppose \( \vec{w} \) is a linear combination of \( \vec{u_1},\vec{u_2},\dots,
\vec{u_k} \) and each \( \vec{u_1} \) is a linear combination of \( \vec{v_1},
\dots,\vec{v_m} \). Prove \( \vec{w} \) is a linear combination of \( \vec{v_1},
\dots,\vec{v_m} \).
\begin{align*}
  \vec{w} &= \sum_{i=1}^{k}c_iu_i \\
  &= \sum_{i=1}^{k}c_i\left(\sum_{j=1}^{m}d_{ij}\vec{v_j}\right) \\
  &= \sum_{i=1}^{k}\left(\sum_{j=1}^{m}c_id_{ij}\vec{v_j}\right) \\
  &= \sum_{j=1}^{m}\left(\sum_{i=1}^{k}c_id_{ij}\right)\vec{v_i} \\
  w &\in span(\vec{v_1},\dots,\vec{v_m})
\end{align*}
Also suppose each \( \vec{v_j} \) is a linear combination of \( \vec{u_1},\dots,
\vec{u_k} \). Prove \( span(\vec{u_1},\dots,\vec{u_k}) =
span(\vec{v_1},\dots,\vec{v_m}) \). Above we proved:
\[ span(\vec{u_1},\dots,\vec{u_k}) \subseteq span(\vec{v_1},\dots,\vec{v_m}) \]
Therefore:
\[ span(\vec{u_1},\dots,\vec{u_k}) \supseteq span(\vec{v_1},\dots,\vec{v_m}) \]

\subsubsection*{Example}
If the columns of an \( n\times n \) matrix \( A \) are linearly independent
as vectors in \( \R^n \), what is the rank of \( A \)?
\[ rank(A) = n \]
The columns are linearly independent as \( A\vec{x} = \vec{0} \) has only the
trivial solution. So there are no free variables.

\subsubsection*{Example}
Prove two vectors are linearly dependent if and only if 1 is a scalar multiple
of the other. \\
Say \( \vec{u} \) and \( \vec{v} \) are linearly depedent, there exists scalars
\( c_1,c_2 \) (not both 0) such that \( c_1\vec{u}+c_2\vec{v} = \vec{0} \).
Without loss of generality, we say that \( c_1 \ne 0 \), then:
\begin{align*}
  c_1\vec{u} &= -c_2\vec{v} \\
  \vec{u} &= \frac{-c_2}{c_1}\vec{v}
\end{align*}
Suppose \( \vec{u} \) is a scalar multiple of \( \vec{v} \).
\begin{align*}
  \vec{u} &= c\vec{v} \\
  c\vec{v}-\vec{u} &= \vec{0}
\end{align*}
Thus, \( \vec{u} \) and \( \vec{v} \) are linearly dependent.

\begin{center}
  You can find all my notes at \url{http://omgimanerd.tech/notes}. If you have
  any questions, comments, or concerns, please contact me at
  alvin@omgimanerd.tech
\end{center}

\end{document}

\documentclass[letterpaper, 12pt]{math}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

\title{Linear Algebra}
\author{Alvin Lin}
\date{August 2017 - December 2017}

\begin{document}

\maketitle

\section*{Linear Algebra}
The study of linear algebra is about two basic things. We study
\textbf{vector spaces} and structure preserving maps between vector
spaces. A \textbf{vector space} is set \( v \) with two binary operations,
addition and scalar multiplcation. A vector space will satisfy various
distributive identities.
\[ \R^2, \R^3, \dots, \R^n \]
are all vector spaces.

\section*{Vectors}
A \textbf{vector} is a directed line segment corresponding to the displacement
from points \( A \) to \( B \) (in \( \R^2 \)). If a vector starts at the
origin, we will say that the vector is in standard position. We can represent
them as row vectors or column vectors depending on convenience. Each is an
ordered pair.
\[ \vec{v} =
  \begin{bmatrix}
    2 & 3 \\
  \end{bmatrix} \]
\[ \vec{v} =
  \begin{bmatrix}
    2 \\
    3
  \end{bmatrix}
\]
The zero vector (\( \vec{0} \)) is a special vector.
\[ \vec{0} =
  \begin{bmatrix}
    0 & 0
  \end{bmatrix}
\]
Let \( \vec{v}\in\R^2 \):
\[ \vec{v}+\vec{0}=\vec{0}+\vec{v}=\vec{v} \]

\subsection*{Adding vectors in \( \R^2 \)}
Take the following:
\begin{align*}
  \vec{v} &= \langle v_{1},v_{2}\rangle \\
  \vec{w} &= \langle w_{1},w_{2}\rangle \\
  \vec{v}+\vec{w} &= \langle v_{1}+w_{1},v_{2}+w_{2}\rangle
\end{align*}
Example:
\begin{align*}
  \vec{v} &= \langle1,2\rangle \\
  \vec{w} &= \langle3,4\rangle \\
  \vec{v}+\vec{w} &= \langle1+3,2+4\rangle = \langle4,6\rangle
\end{align*}

\subsection*{Scalar Multiplication}
Let \( c \) be a scalar and let \( \vec{v} = \langle v_{1},v_{2}\in\R^{2} \).
\[ c\cdot\vec{v} = c\langle v_{1},v_{2}\rangle = \langle cv_{1},cv_{2}\rangle \]
Example:
\[ 2\cdot\langle3,4\rangle = \langle2(3),2(4)\rangle = \langle6,8\rangle \]
If \( c>0 \), then \( c\cdot\vec{v} \) points in the same direction as
\( \vec{v} \). \\
If \( c<0 \), then \( c\cdot\vec{v} \) points in the opposite
direction as \( \vec{v} \). \\
If \( |c|>1 \), then \( c\cdot\vec{v} \) is \( \vec{v} \) stretched by a factor
of \( c \). \\
If \( |c|<1 \), then \( c\cdot\vec{v} \) is \( \vec{v} \) compressed by a factor
of \( c \).

\subsection*{Vectors in \( \R^{3} \)}
\[ \R^{3} = \{(x,y,z)\ |\ x,y,z\in\R\} \]
Similarly, vectors in \( \R^{n} \) are:
\[ \R^{n} = \{(x_{1},x_{2},\dots,x_{n})\ |\ all\ x_{i}\in\R\} \]
Let \( \vec{u},\vec{v}\in\R^{n} \), let \( c\in\R \):
\begin{align*}
  \vec{u}+\vec{v} &= \langle u_{1},u_{2},\dots,u_{n}\rangle +
    \langle v_{1},v_{2},\dots,v_{n}\rangle +
    \langle u_{1}+v_{1},u_{2}+v_{2},\dots,u_{n}+v_{n}\rangle \\
  c\vec{v} &= c\langle v_{1},v_{2},\dots,v_{n}\rangle =
    \langle cv_{1},cv_{2},\dots,cv_{n}\rangle
\end{align*}

\subsection*{Algebraic Properties of \( \R^n \)}
Let:
\begin{align*}
  \vec{u} &= \langle u_{1},u_{2},\dots,u_{n}\rangle\in\R^{n} \\
  \vec{v} &= \langle v_{1},v_{2},\dots,v_{n}\rangle\in\R^{n} \\
  \vec{w} &= \langle w_{1},w_{2},\dots,w_{n}\rangle\in\R^{n}
\end{align*}
Let \( c,d\in\R \) (scalars):
\begin{enumerate}
  \item Commutative
    \[ \vec{u}+\vec{v} = \vec{v}+\vec{u} \]
  \item Associative
    \[ (\vec{u}+\vec{v})+\vec{w} = \vec{u}+(\vec{v}+\vec{w}) \]
  \item
    \[ \vec{u}+\vec{0} = \vec{0}+\vec{u} = \vec{u} \]
  \item
    \[ \vec{u}+(-\vec{u}) = \vec{0} \]
  \item Distributive
    \[ c(\vec{u}+\vec{v}) = c\vec{u}+c\vec{v} \]
  \item
    \[ (c+d)\vec{u} = c\vec{u}+d\vec{u} \]
  \item
    \[ c(d\vec{u}) = (cd)\vec{u} \]
  \item
    \[ 1\vec{u} = \vec{u} \]
\end{enumerate}
Proof of (1):
\begin{align*}
  \vec{u}+\vec{v} &= \langle u_{1},u_{2},\dots,u_{n}\rangle+
    \langle v_{1},v_{2},\dots,v_{n}\rangle \\
  &= \langle u_{1}+v_{1},u_{2}+v_{2},\dots,u_{n}+v_{n}\rangle \\
  &= \langle v_{1}+u_{1},v_{2}+u_{2},\dots,v_{n}+u_{n}\rangle \\
  &= \langle v_{1},v_{2},\dots,v_{n}\rangle+
    \langle u_{1},u_{2},\dots,u_{n}\rangle \\
  &= \vec{v}+\vec{u}
\end{align*}
Proof of (2):
\begin{align*}
  (\vec{u}+\vec{v})+\vec{w} &= (\langle u_{1},\dots,u_{n}\rangle+
    \langle v_{1},\dots,v_{n}\rangle)+\langle w_{1},\dots,w_{n}\rangle \\
  &= \langle u_{1}+v_{1},\dots,u_{n}+v_{n}\rangle+
    \langle w_{1},\dots,w_{n}\rangle \\
  &= \langle(u_{1}+v_{1})+w_{1},\dots,(u_{n}+v_{n})+w_{n}\rangle \\
  &= \langle u_{1},\dots,u_{n}\rangle +
    (\langle v_{1}+w_{1},\dots,v_{n}+w_{n}) \\
  &= \vec{u}+(\langle v_{1},\dots,v_{n}\rangle+
    \langle w_{1},\dots,w_{n}\rangle) \\
  &= \vec{u}+(\vec{v}+\vec{w})
\end{align*}
Proof of (3):
\begin{align*}
  \vec{u}+\vec{0} &= \langle u_{1},u_{2},\dots,u_{n}\rangle+
    \langle0,0,\dots,0\rangle \\
  &= \langle u_{1}+0,u_{2}+0,\dots,u_{n}+0\rangle \\
  &= \langle u_{1},u_{2},\dots,u_{n}\rangle = \vec{u}
\end{align*}
Proof of (4):
\begin{align*}
  \vec{u}+(-\vec{u}) &= \langle u_{1},u_{2},\dots,u_{n}\rangle+
    \langle-u_{1},-u_{2},\dots,-u_{n}\rangle \\
  &= \langle u_{1}-u_{1},u_{2}-u_{2},\dots,u_{n}-u_{n}\rangle \\
  &= \langle0,0,\dots,0\rangle = \vec{0}
\end{align*}
Proof of (5):
\begin{align*}
  c(\vec{u}+\vec{v}) &= c(\langle u_{1},u_{2},\dots,u_{n}\rangle+
    \langle v_{1},v_{2},\dots,v_{n}\rangle) \\
  &= c(\langle u_{1}+v_{1},u_{2}+v_{2},\dots,u_{n}+v_{n}\rangle) \\
  &= \langle c(u_{1}+v_{1}),c(u_{2}+v_{2}),\dots,c(u_{n}+v_{n})\rangle \\
  &= \langle cu_{1}+cv_{1},cu_{2}+cv_{2},\dots,cu_{n}+cv_{n}\rangle \\
  &= \langle cu_{1},cu_{2},\dots,cu_{n}\rangle+
    \langle cv_{1},cv_{2},\dots,cv_{n}\rangle \\
  &= c\langle u_{1},u_{2},\dots,u_{n}\rangle+
    c\langle v_{1},v_{2},\dots,v_{n}\rangle \\
  &= c\vec{u}+c\vec{v}
\end{align*}

\subsection*{Linear combinations}
Let \( \vec{v_{1}},\vec{v_{2}},\dots,\vec{v_{k}}\in\R^n \). We say
\( \vec{v} \) is a \textbf{linear combination} of
\( \vec{v_{1}},\dots,\vec{v_{k}} \) if there exists scalars
\( c_{1},\dots,c_{k} \) such that:
\[ \vec{v} = \sum_{i=1}^{k}c_{i}\vec{v_{i}} =
  c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+\dots+c_{n}\vec{v_{n}} \]
Example in \( \R^{3} \):
Let:
\begin{align*}
  \vec{v} &=
    \begin{bmatrix}
      2 \\ -2 \\ -1
    \end{bmatrix} \quad
  \vec{v_{1}} =
    \begin{bmatrix}
      1 \\ 0 \\ -1
    \end{bmatrix} \\
  \vec{v_{2}} &=
    \begin{bmatrix}
      2 \\ -3 \\ 1
    \end{bmatrix} \quad
  \vec{v_{3}} =
    \begin{bmatrix}
      5 \\ -4 \\ 0
    \end{bmatrix}
\end{align*}
Claim: \( \vec{v} \) is a linear combination of
\( \vec{v_{1}},\vec{v_{2}},\vec{v_{3}} \): \\
We must find \( c_{1},c_{2},c_{3} \) such that:
\[ \vec{v} = c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+c_{3}\vec{v_{3}} \]

\subsection*{Dot Product}
Let:
\begin{align*}
  \vec{u} &=
    \begin{bmatrix}
      u_{1} \\ u_{2} \\ \vdots \\ u_{n}
    \end{bmatrix} \\
  \vec{v} &=
    \begin{bmatrix}
      v_{1} \\ v_{2} \\ \vdots \\ v_{n}
    \end{bmatrix} \\
  \vec{u}\cdot\vec{v} &= \sum_{i=1}^{n}u_{i}v_{i} \\
  &= u_{1}v_{1}+u_{2}v_{2}+\dots+u_{n}v_{n}
\end{align*}

\subsection*{Rules for the dot product}
Let \( \vec{u},\vec{v},\vec{w}\in\R^n \), \( c \) is a scalar:
\begin{enumerate}
  \item
    \[ \vec{u}\cdot\vec{v} = \vec{v}\cdot\vec{u} \]
  \item
    \[ \vec{u}\cdot(\vec{v}+\vec{w}) =
      \vec{u}\cdot\vec{v}+\vec{u}\cdot\vec{w} \]
  \item
    \[ (c\vec{u})\cdot\vec{v} = c(\vec{u}\cdot\vec{v}) \]
  \item
    \[ \vec{u}\cdot\vec{u}\geq0 \]
    \[ \vec{u}\cdot\vec{u} = 0\ \textrm{iff}\ \vec{u} = \vec{0} \]
\end{enumerate}
Proof for (1):
\begin{align*}
  \vec{u}\cdot\vec{v} &= \sum_{i=1}^{n}u_{i}v_{i} \\
  &= \sum_{i=1}^{n}v_{i}u_{i} \\
  &= \vec{v}\cdot\vec{u}
\end{align*}
Proof for (2):
\begin{align*}
  \vec{u}\cdot(\vec{v}+\vec{w}) &=
    (\langle v_{1}+w_{1},\dots,v_{n}+w_{n}\rangle) \\
  &= \sum_{i=1}^{n}u_{i}(v_{i}+w_{i}) \\
  &= \sum_{i=1}^{n}(u_{i}v_{i}+u_{i}+w_{i}) \\
  &= \sum_{i=1}^{n}u_{i}v_{i}+\sum_{i=1}^{n}u_{i}w_{i} \\
  &= \vec{u}\cdot\vec{v}+\vec{u}\cdot\vec{w}
\end{align*}
Proof for (3):
\begin{align*}
  \sum_{i=1}^{n}(cu_{i})v_{i} &= \sum_{i=1}^{n}c(u_{i}v_{i}) \\
  &= c\sum_{i=1}^{n}u_{i}v_{i} \\
  &= c(\vec{u}\cdot\vec{v})
\end{align*}
Proof for (4):
\begin{align*}
  \vec{u}\cdot\vec{u} &= \sum_{i=1}^{n}u_{i}u_{i} \\
  &= \sum_{i=1}^{n}(u_{1})^{2}
\end{align*}
\( (u_{1})^{2} \) is non-negative, therefore the summation must be greater than
or equal to 0.

\subsection*{Norm or length of a vector}
Let:
\begin{align*}
  \vec{v}\in\R^n \quad \vec{v} &= \begin{bmatrix}
    v_{1} \\ v_{2} \\ \vdots \\ v_{N}
  \end{bmatrix} \\
  \|\vec{v}\| &= \sqrt{\vec{v}\cdot\vec{v}} \\
  &= \sqrt{\sum_{i=1}^{n}(v_{1})^{2}}
\end{align*}

\subsubsection*{Example}
In \( \R^2 \), consider \( \vec{v} = \begin{bmatrix}3 \\ 4\end{bmatrix} \).
Calculate \( |\vec{v}| \).
\[ \|\vec{v}\| = \sqrt{3^{2}+4^{2}} = \sqrt{9+16} = \sqrt{25} = 5 \]

\subsubsection*{Properties of a vector norm}
\[ |\vec{v}| = 0 \iff \vec{v} = \vec{0} \]
This is true because \( \|\vec{v}\| = 0 \iff \vec{v}\cdot\vec{v} = 0 \iff
  \vec{v} = \vec{0} \).
\[ \|c\vec{v}\| = |c|\|\vec{v}\| \]
Proof:
\begin{align*}
  \|c\vec{v}\|^{2} &= (c\vec{v})\cdot(c\vec{v}) \\
  &= c^{2}(\vec{v}\cdot\vec{v}) \\
  &= c^{2}\|\vec{v}\|^{2} \\
  \sqrt{\|c\vec{v}\|^{2}} &= \sqrt{c^{2}\|\vec{v}\|^{2}} \\
  \|\vec{v}\| = |c|\|\vec{v}\|
\end{align*}

\subsection*{Unit Vectors}
A \textbf{unit vector} is a vector of length 1. In \( \R^2 \):
\begin{align*}
  \vec{e_{1}} &= \begin{bmatrix}
    1 \\ 0
  \end{bmatrix} \\
  \vec{e_{2}} &= \begin{bmatrix}
    0 \\ 1
  \end{bmatrix}
\end{align*}
In \( \R^3 \):
\begin{align*}
  \vec{e_{1}} &= \begin{bmatrix}
    1 \\ 0 \\ 0
  \end{bmatrix} \\
  \vec{e_{2}} &= \begin{bmatrix}
    0 \\ 1 \\ 0
  \end{bmatrix} \\
  \vec{e_{3}} &= \begin{bmatrix}
    0 \\ 0 \\ 1
  \end{bmatrix}
\end{align*}
In \( \R^n \), there exist the unit vectors \( \vec{e_{1}},\vec{e_{2}},\dots,
\vec{e_{n}} \). \( e_{i} \) has a 1 in the \( i^th \) component and zeros
everywhere else.

\subsection*{Vector Normalization}
In \( \R^3 \) with \( \vec{v}\in\R^{n} \) and \( \vec{v}\neq\vec{0} \), the
unit vector corresponding to \( \vec{v} \) is:
\[ \vec{v}_{norm} = \frac{1}{\|\vec{v}\|}\vec{v} \]

\subsubsection*{Example}
Normalize \( \vec{v} = \begin{bmatrix}2 \\ -1 \\ -3\end{bmatrix} \):
\begin{align*}
  \|\vec{v}\| &= \sqrt{2^2+(-1)^2+(-3)^2} \\
  &= \sqrt{4+1+9} \\
  &= \sqrt{14} \\
  \vec{v}_{norm} &= \frac{1}{\|\vec{v}\|}\vec{v} \\
  &= \frac{1}{\sqrt{14}}\begin{bmatrix}2 \\ -1 \\ -3\end{bmatrix} \\
  &= \begin{bmatrix}
    \frac{2}{\sqrt{14}} \\
    \frac{-1}{\sqrt{14}} \\
    \frac{-3}{\sqrt{14}}
  \end{bmatrix}
\end{align*}

\subsection*{Cauchy-Schwarz Inequality}
Let \( \vec{u},\vec{v}\in\R^{n} \):
\[ |\vec{u}\cdot\vec{v}|\leq\|\vec{u}\|\|\vec{v}\| \]

\subsection*{The Triangle Inequality}
Let \( \vec{u},\vec{v}\in\R^{n} \):
\[ \|\vec{u}+\vec{v}\|\leq\|\vec{u}\|+\|\vec{v}\| \]
Proof:
\begin{align*}
  \|\vec{u}+\vec{v}\|^2 &= (\vec{u}+\vec{v})\cdot(\vec{u}+\vec{v}) \\
  &= \vec{u}\cdot\vec{u}+2\vec{u}\vec{v}+\vec{u}\cdot\vec{v} \\
  &= \|\vec{u}\|^2+2\vec{u}\cdot\vec{v}+\|\vec{v}\|^2 \\
  & \textrm{By the Cauchy-Schwarz Inequality} \\
  &\leq \|\vec{u}\|^2+2\|\vec{u}\|\|\vec{v}\|+\|\vec{v}\|^2 \\
  &= (\|\vec{u}\|+\|\vec{v}\|)^2
\end{align*}
Thus, \( \|\vec{u}+\vec{v}\|^2 \leq (\|\vec{u}\|+\|\vec{v}\|)^2 \). Taking the
square root:
\[ \|\vec{u}+\vec{v}\|\leq\|\vec{u}\|+\|\vec{v}\| \]

\subsection*{Distance Formula}
In \( \R \), let \( a,b\in\R \). Find the formula for \( d(a,b) \):
\[ d(a,b) = |a-b| = \sqrt{(a-b)^2}\]
In \( \R^2 \), let \( \vec{v_{1}} = [a_{1},b_{1}] \), and \( v_{2} = [a_{2},
b_{2}] \).
\begin{align*}
  d(\vec{v_{1}},\vec{v_{2}}) &= \|\vec{v_{1}}-\vec{v_{2}}\| \\
  &= \sqrt{(a_{1}-a_{2})^2-(b_{2}-b_{2})^2}
\end{align*}
In \( \R^n \), let \( \vec{v},\vec{w} \) be vectors. Define the distance from
\( \vec{v} \) to \( \vec{w} \) as follows:
\begin{align*}
  d(\vec{v},\vec{w}) &= \|\vec{v}-\vec{w}\| \\
  &= \sqrt{\sum_{i=1}^{n}(v_{1}-w_{1})^2}
\end{align*}

\subsubsection*{Example}
\begin{align*}
  \vec{u} &= \begin{bmatrix}\sqrt{2} \\ 1 \\ -1\end{bmatrix} \\
  \vec{v} &= \begin{bmatrix}0 \\ 2 \\ -2\end{bmatrix} \\
  d(\vec{u},\vec{v}) &= \sqrt{(\sqrt{2}-0)^2+(1-2)^2+(-1+2)^2} \\
  &= \sqrt{2+(-1)^2+1^2} \\
  &= \sqrt{4} \\
  &= 2
\end{align*}

\subsection*{Angle Between Vectors}
If we have two vectors \( \vec{v},\vec{u} \) and the angle \( \theta \) between
them, we can use the law of cosines to find \( \theta \):
\begin{align*}
  \|\vec{u}-\vec{v}\|^2 &=
    \|\vec{u}\|^2+\|\vec{v}\|^2-2\|\vec{u}\|\|\vec{v}\|\cos\theta \\
  &= (\vec{u}-\vec{v})\cdot(\vec{u}-\vec{v}) \\
  &= \vec{u}\cdot\vec{u}-2\vec{u}\cdot\vec{v}+\vec{v}\cdot\vec{v} \\
  &= \|\vec{u}\|^2-2\vec{u}\cdot\vec{v}+\|\vec{v}\|^2 \\
  \|\vec{u}\|^2-2\vec{u}\cdot\vec{v}+\|\vec{v}\|^2 &=
    \|\vec{u}\|^2+\|\vec{v}\|^2-2\|\vec{u}\|\|\vec{v}\|\cos\theta \\
  -2\vec{u}\cdot\vec{v} &= -2\|\vec{u}\|\|\vec{v}\|\cos\theta \\
  \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|} &= \cos\theta
\end{align*}
Let \( \vec{u},\vec{v}\in\R^n \), the angle \( \theta \) between \( \vec{v} \)
and \( \vec{u} \) is:
\[ \cos\theta = \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|} \]

\subsubsection*{Example}
Find the angle between:
\begin{align*}
  \vec{u} &= \begin{bmatrix}2 \\ 1 \\ -2\end{bmatrix} \\
  \vec{v} &= \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix} \\
  \vec{u}\cdot\vec{v} &= 2(1)+1(1)+(-2)(1) \\
  &= 2+1-2 = 1 \\
  \|\vec{u}\| &= \sqrt{2^2+1^2+(-2)^2} = \sqrt{9} = 3 \\
  \|\vec{v}\| &= \sqrt{1^2+1^2+1^2} = \sqrt{3} \\
  \cos\theta &= \vec{1}{3\sqrt{3}} \\
  \theta &\approx 1.377rad \approx 78.9^\circ
\end{align*}

\subsection*{Orthogonal Vectors}
We say two vectors \( \vec{u} \) and \( \vec{v} \) are orthogonal if there's
a right angle between them.
\begin{align*}
  \cos(\frac{\pi}{2}) &= \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|} \\
  0 &= \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|} \\
  &\therefore \vec{u}\cdot\vec{v} = 0
\end{align*}
Two vectors are orthogonal if their dot product is zero.

\subsubsection*{Example}
\begin{align*}
  \vec{u} &= \langle1,1,-2\rangle \\
  \vec{v} &= \langle3,1,2\rangle \\
  \vec{u}\cdot\vec{v} &= 1(3)+1(1)+(-2)(2) \\
  &= 3+1-4 = 0
\end{align*}

\subsection*{The Pythagorean Theorem}
For vectors \( \vec{u},\vec{v}\in\R^n \), suppose \( \vec{u} \) and
\( \vec{v} \) are orthogonal. Then:
\begin{align*}
  \|\vec{u}+\vec{v}\|^2 &= \|\vec{u}\|^2+\|\vec{v}\|^2 \\
  (\vec{u}+\vec{v})\cdot(\vec{u}+\vec{v}) &=
    \vec{u}\cdot\vec{u}+2\vec{u}\cdot\vec{v}+\vec{v}\cdot\vec{v} \\
  &= \|\vec{u}\|^2+2\vec{u}\cdot\vec{v}+\|\vec{v}\|^2 \\
  &= \|\vec{u}\|^2+0+\|\vec{v}\|^2 \\
  &= \|\vec{u}\|^2+\|\vec{v}\|^2
\end{align*}

\subsection*{Vector Projection}
The projection of \( \vec{v} \) onto \( \vec{u} \) described by \( \vec{p} \)
is:
\begin{center}
  \begin{tikzpicture}
    \draw[->] (0,0)--(4,0) node[pos=0.5,below] {\( \vec{v} \)};
    \draw[->] (0,0)--(1,2) node[pos=0.5,left] {\( \vec{u} \)};
    \draw[->,red,thick] (0,0)--(1,0) node[pos=0.5,below]
      {\( \vec{p} = proj_{\vec{v}}\vec{u} \)};
    \draw[dashed] (1,2)--(1,0);
  \end{tikzpicture}
\end{center}
\[ proj_{\vec{u}}\vec{v} =
  (\frac{\vec{u}\cdot\vec{v}}{\vec{u}\cdot\vec{u}})\vec{u} =
  (\frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|^2})\vec{u} \]

\subsubsection*{Practice Problem 48}
Given:
\[ \vec{u} = \begin{bmatrix}2 \\ 3\end{bmatrix}
  \vec{v} = \begin{bmatrix}k+1 \\ k-1\end{bmatrix} \]
Find all values of \( k \) such that \( \vec{u} \) and \( \vec{v} \) are
orthogonal.
\begin{align*}
  \vec{u}\cdot\vec{v} &= 0 \\
  2(k+1)+3(k-1) &= 0 \\
  2k+2+3k-3 &= 0 \\
  5k-1 &= 0 \\
  k &= \frac{1}{5}
\end{align*}

\subsubsection*{Practice Problem 49}
Given:
\[ \vec{u} = \begin{bmatrix}1 \\ -1 \\ 2\end{bmatrix}
  \vec{v} = \begin{bmatrix}k^2 \\ k \\ -3\end{bmatrix} \]
Find all values \( k \) such that \( \vec{u} \) and \( \vec{v} \) are
orthogonal.
\begin{align*}
  \vec{u}\cdot\vec{v} &= 0 \\
  k^2-k+2(-3) &= 0 \\
  k^2-k-6 &= 0 \\
  (k-3)(k+2) &= 0 \\
  k &= 3\ or \\
  k &= 2
\end{align*}

\subsubsection*{Practice Problem 50}
Describe all vectors \( \vec{v} = \begin{bmatrix}x \\ y\end{bmatrix} \) that
are orthogonal to \( \vec{u} = \begin{bmatrix}3 \\ 1\end{bmatrix} \).
\begin{align*}
  \vec{u}\cdot\vec{v} &= 0 \\
  3x+y &= 0 \\
  y &= -3x
\end{align*}
\( y \) is a line going through the origin with a slope of -3.

\subsubsection*{Practice Problem 52}
Under what conditions are the following true for vectors \( \vec{u},\vec{v}
\in\R^2\ or\ \R^3 \).
\[ \|\vec{u}+\vec{v}\| = \|\vec{u}\|+\|\vec{v}\| \]
\begin{align*}
  \|\vec{u}+\vec{v}\|^2 &= (\|\vec{u}\|+\|\vec{v}\|)^2 \\
  (\vec{u}+\vec{v})\cdot(\vec{u}+\vec{v}) &= \|\vec{u}\|^2+
    2\|\vec{u}\|\|\vec{v}\|+\|\vec{v}\|^2 \\
  \vec{u}\cdot\vec{u}+2\vec{u}\cdot\vec{v}+\vec{v}\cdot\vec{v} &=
    \|\vec{u}\|^2+2\|\vec{u}\|\|\vec{v}\|+\|\vec{v}\|^2 \\
  \vec{u}\cdot\vec{v} &= \|\vec{u}\|\|\vec{v}\|
\end{align*}
Since:
\[ \cos\theta = \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|} \]
\( \theta \) must be equal to zero since \( \vec{u}\cdot\vec{v} =
\|\vec{u}\|\|\vec{v}\| \). Therefore, this can only be true for \( \vec{u} =
c\vec{v}, c > 0 \).

\subsubsection*{Practice Problem 55}
Verify the stated property of distances:
\[ d(\vec{u},\vec{v}) = \|\vec{u}-\vec{v}\| \]
\begin{align*}
  d(\vec{u},\vec{v}) &= \|\vec{u}-\vec{v}\| \\
  &= \|(-1)(\vec{v}-\vec{u})\| \\
  &= |-1|\|\vec{v}-\vec{u}\| \\
  &= d(\vec{v},\vec{u})
\end{align*}

\subsubsection*{Practice Problem 56}
\[ d(\vec{u},\vec{w}) \le d(\vec{u},\vec{v})+d(\vec{v},\vec{w}) \]
\begin{align*}
  d(\vec{u},\vec{w}) &= \|\vec{u}-\vec{w}\| \\
  &= \|(\vec{u}-\vec{v})+(\vec{v}-\vec{w})\| \\
  &\le \|\vec{u}-\vec{w}\|+\|\vec{v}+\vec{w}\| \\
  &= d(\vec{u},\vec{v})+d(\vec{v},\vec{w})
\end{align*}

\subsubsection*{Practice Problem 58}
Prove:
\[ \vec{u}\cdot c\vec{v} = c(\vec{u}\cdot\vec{v}) \]
\begin{align*}
  \vec{u}\cdot(c\vec{v}) &= (c\vec{v})\cdot\vec{u} \\
  &= c(\vec{v}\cdot\vec{u}) \\
  &= c(\vec{u}\cdot\vec{v})
\end{align*}

\begin{center}
  If you have any questions, comments, or concerns, please contact me at
  alvin@omgimanerd.tech
\end{center}

\end{document}

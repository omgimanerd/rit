\documentclass{math}

\usepackage{enumerate}

\title{Linear Algebra}
\author{Alvin Lin}
\date{August 2017 - December 2017}

\begin{document}

\maketitle

\section*{Eigenvectors and Eigenvalues}
Let \( A \) be an \( n\times n \) matrix. A scalar \( \lambda \) is called an
\textbf{eigenvalue} of \( A \) if there exists a non-zero vector \( \vec{x}\in
\R^n \) such that \( A\vec{x} = \lambda\vec{x} \). Such a vector \( \vec{x} \)
is called an \textbf{eigenvector} of \( A \) corresponding to \( \lambda \). \\
In \( \R^2 \), \( A\vec{x} = \lambda x \) means that the action of \( A \) on
\( \vec{x} \) just yields a vector parallel to \( \vec{x} \).

\subsection*{Properties}
\begin{itemize}
  \item \( A\vec{x} = \lambda\vec{x} \) for some \( \vec{x}\ne\vec{0} \).
  \item \( (A-\lambda I)\vec{x} = \vec{0} \) for some \( \vec{x}\ne\vec{0} \).
  \item \( null(A-\lambda I) \) is nontrivial.
  \item \( \det(A-\lambda I) = 0 \) (allows us to solve for \( \lambda \) to
  find eigenvalues).
  \item \( |A-\lambda I| = 0 \) is called the characteristic polynomial.
\end{itemize}
We define \( E_{\lambda} = null(A-\lambda I) \) to be the eigenspace
corresponding to \( \lambda \). Eigenvalues are the roots of \( |A-\lambda I| \)
and eigenspaces \( E_{\lambda_i} = null(A-\lambda_iI) \).

\subsubsection*{Example}
Show that \( \vec{x} = \begin{bmatrix}1 \\ 1\end{bmatrix} \) is an
eigenvector for \( A = \begin{bmatrix}3 & 1 \\ 1 & 3\end{bmatrix} \).
\begin{align*}
  A\vec{x} &= \begin{bmatrix}
    3 & 1 \\
    1 & 3
  \end{bmatrix}\begin{bmatrix}1 \\ 1\end{bmatrix}
    = \begin{bmatrix}4 \\ 4\end{bmatrix}
    = 4\begin{bmatrix}1 \\ 1\end{bmatrix} \\
  &= 4\lambda
\end{align*}
So \( \lambda = 4 \) is an eigenvalue for \( A \) and \( \vec{x} \) is a
corresponding eigenvector.

\subsubsection*{Example}
Show there exists a non-zero vector \( \vec{x} \) satisfying \( A\vec{x} =
5\vec{x} \).
\[ A = \begin{bmatrix}1 & 2 \\ 4 & 3\end{bmatrix} \]
Conclude: \( \lambda = 5 \) is an eigenvalue of \( A \).
\begin{align*}
  A\vec{x} &= 5\vec{x} \\
  (A-5I)\vec{x} &= \vec{0} \\
  A-5I &= \begin{bmatrix}-4 & 2 \\ 4 & -2\end{bmatrix}
    \sim \begin{bmatrix}-4 & 2 \\ 0 & 0\end{bmatrix}
    \sim \begin{bmatrix}1 & \frac{-1}{2} \\ 0 & 0\end{bmatrix} \\
  x_1-\frac{1}{2}x_2 &= 0 \\
  x_1 &= \frac{1}{2}x_2 \\
  \vec{x} &= \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
    = x_1\begin{bmatrix}1 \\ 2\end{bmatrix} \\
  E_5 &= span\left(\begin{bmatrix}1 \\ 2\end{bmatrix}\right)
\end{align*}

\subsubsection*{Example}
Show that \( \lambda = 6 \) is an eigenvalue of:
\[ A = \begin{bmatrix}7 & 1 & -2 \\ -3 & 3 & 6 \\ 2 & 2 & 2\end{bmatrix} \]
We need to show \( (A-6I)\vec{x} = 6 \) has a non-trivial solution.
\begin{align*}
  A-6I &= \begin{bmatrix}
    1 & 1 & -2 \\
    -3 & -3 & 6 \\
    2 & 2 & -4
  \end{bmatrix} \sim \begin{bmatrix}
    1 & 1 & -2 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
\end{align*}
\begin{align*}
  x_1+x_2-2x_3 &= 0 \\
  x_1 &= -x_2+2x_3 \\
  \vec{x} &= \begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix}
    = \begin{bmatrix}-x_2+2x_3 \\ x_2 \\ x_3\end{bmatrix}
    = x_2\begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix}+
    x_2\begin{bmatrix}2 \\ 0 \\ 0\end{bmatrix} \\
  E_6 &= span\left(\begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix},
    \begin{bmatrix}2 \\ 0 \\ 0\end{bmatrix}\right)
\end{align*}

\subsubsection*{Example}
Find the eigenvectors and eigenvalues over \( \R \) and \( \mathbb{C} =
\{a+bi\mid a,b\in\R\}, i = \sqrt{-1} \) of the following:
\begin{enumerate}[(i)]
  \item \( A = \begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix} \)
  \begin{align*}
    A-\lambda I &= \begin{bmatrix}
      1-\lambda & 0 \\
      0 & -1-\lambda
    \end{bmatrix} \\
    (1-\lambda)(-1-\lambda) &= 0 \\
    1-\lambda &= 0 \\
    -1-\lambda &= 0 \\
    \lambda &= \pm1 \\
  \end{align*}
  For \( \lambda = 1 \):
  \begin{align*}
    A-I &= \begin{bmatrix}0 & 0 \\ 0 & -1\end{bmatrix}
      \sim \begin{bmatrix}0 & 0 \\ 0 & 1\end{bmatrix} \\
    \vec{x} &= \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
      = \begin{bmatrix}x_1 \\ 0\end{bmatrix}
      = x_1\begin{bmatrix}1 \\ 0\end{bmatrix} \\
    E_1 &= span\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right)
  \end{align*}
  For \( \lambda = -1 \):
  \begin{align*}
    A+I &= \begin{bmatrix}2 & 0 \\ 0 & 0\end{bmatrix}
      \sim \begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix} \\
    \vec{x} &= \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
      = \begin{bmatrix}0 \\ x_2\end{bmatrix}
      = x_2\begin{bmatrix}0 \\ 1\end{bmatrix} \\
    E_1 &= span\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right)
  \end{align*}
  \item \( B = \begin{bmatrix}3 & 1 \\ 1 & 3\end{bmatrix} \)
  \begin{align*}
    B-\lambda I &= \begin{bmatrix}
      3-\lambda & 1 \\
      1 & 3-\lambda
    \end{bmatrix} \\
    (3-\lambda)(3-\lambda)-1 &= 0 \\
    9-6\lambda+\lambda^2-1 &= 0 \\
    (\lambda-4)(\lambda-2) &= 0 \\
    \lambda = 4 &\quad \lambda = 2
  \end{align*}
  For \( \lambda = 2 \):
  \begin{align*}
    B-2I &= \begin{bmatrix}1 & 1 \\ 1 & 1\end{bmatrix}
      \sim \begin{bmatrix}1 & 1 \\ 0 & 0\end{bmatrix} \\
    x_1+x_2 &= 0 \\
    x_1 &= -x_2 \\
    \vec{x} &= \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
      = \begin{bmatrix}-x_2 \\ x_2\end{bmatrix}
      = x_2\begin{bmatrix}-1 \\ 1\end{bmatrix} \\
    E_2 &= span\left(\begin{bmatrix}-1 \\ 1\end{bmatrix}\right)
  \end{align*}
  For \( \lambda = 4 \):
  \begin{align*}
    B-4I &= \begin{bmatrix}-1 & 1 \\ 1 & -1\end{bmatrix}
      \sim \begin{bmatrix}-1 & 1 \\ 0 & 0\end{bmatrix}
      \sim \begin{bmatrix}1 & -1 \\ 0 & 0\end{bmatrix} \\
    x_1-x_2 &= 0 \\
    x_1 &= x_2 \\
    \vec{x} &= \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
      = \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
      = x_1\begin{bmatrix}1 \\ 1\end{bmatrix} \\
    E_4 &= span\left(\begin{bmatrix}1 \\ 1\end{bmatrix}\right)
  \end{align*}
  \item \( C = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix} \)
  \begin{align*}
    C-\lambda I &= \begin{bmatrix}
      -\lambda & -1 \\
      1 & -\lambda
    \end{bmatrix} \\
    (-\lambda)^2-(1)(-1) &= 0 \\
    \lambda^2+1 &= 0 \\
    \lambda^2 &= -1 \\
    \lambda &= \pm i
  \end{align*}
  Depending on our domain of interest, we have no real-valued eigenvalues and
  complex eigenvalues \( \pm i \).
\end{enumerate}

\subsection*{Theorems}
\begin{enumerate}[Theorem 1.]
  \item The eigenvalues of a triangular matrix are the entries on its main
  diagonal. \\
  \textbf{Proof:}
  \begin{align*}
    A &= \begin{bmatrix}
      a_{11} & \dots & \dots & \dots \\
      0 & a_{22} & \dots & \dots \\
      \vdots & \vdots & \vdots & \vdots \\
      0 & \dots & \dots & a_{nn}
    \end{bmatrix} \\
    0 &= |A-\lambda I| \\
    &= \begin{vmatrix}
      a_{11}-\lambda & \dots & \dots & \dots \\
      0 & a_{22}-\lambda & \dots & \dots \\
      \vdots & \vdots & \vdots & \vdots \\
      0 & \dots & \dots & a_{nn}-\lambda
    \end{vmatrix} \\
    &= (a_{11}-\lambda)(a_{22}-\lambda)\dots(a_{nn}-\lambda) \\
    \lambda &= a_{11},a_{22},\dots,a_{nn}
  \end{align*}
  \item A square matrix \( A \) is invertible if and only if \( 0 \) is not
  an eigenvalue of \( A \). \\
  \textbf{Proof:}
  \begin{align*}
    A \text{ is invertible} &\leftrightarrow |A|\ne0 \\
    &\leftrightarrow |A-0I|\ne0 \\
    &\leftrightarrow \lambda = 0 \text{ is not an eigenvalue of } A
  \end{align*}
  \item Let \( A \) be a square matrix with eigenvalue \( \lambda \) and
  corresponding eigenvector \( \vec{x} \), then:
  \begin{enumerate}[(i)]
    \item For any positive integer \( n \), \( \lambda^n \) is an eigenvalue
    of \( A^n \) with corresponding eigenvector \( \vec{x} \). \\
    \textbf{Proof:} \\
    Base Case \( n = 1 \):
    \[ A^1\vec{x} = \lambda^1\vec{x} \]
    Induction Hypothesis:
    \[ A^n\vec{x} = \lambda^n\vec{x} \]
    Induction Step:
    \begin{align*}
      A^{n+1}\vec{x} &= A(A^n\vec{x}) \\
      &= A(\lambda^n\vec{x}) \\
      &= \lambda^n(A\vec{x}) \\
      &= \lambda^n(\lambda\vec{x}) \\
      &= \lambda^{n+1}\vec{x}
    \end{align*}
    \item If \( A \) is invertible, then \( \frac{1}{\lambda} \) is an
    eigenvalue of \( A^{-1} \) with corresponding eigenvector \( \vec{x} \). \\
    \textbf{Proof:}
    \begin{align*}
      \lambda(A^{-1}\vec{x}) &= A^{-1}(\lambda\vec{x}) \\
      &= A^{-1}(A\vec{x}) \\
      &= I\vec{x} \\
      &= \vec{x} \\
      \therefore \lambda(A^{-1}\vec{x}) &= \vec{x} \leftrightarrow
        A^{-1}\vec{x} = (\frac{1}{\lambda})\vec{x}
    \end{align*}
    \item If \( A \) is invertible, the for any integer \( n \), \( \lambda^n \)
    is an eigenvalue of \( A^n \) with corresponding eigenvector
    \( \vec{x} \). \\
    \textbf{Proof:} \\
    We only need to show this for negative integers. \\
    Base Case \( n = -1 \): Because of part (ii), this is true. \\
    Induction Hypothesis:
    \[ A^{-n}\vec{x} = \frac{1}{\lambda^n}\vec{x} \]
    Induction Step: Assume it is true for \( -n \) and show it is true for
    \( -n-1 \).
    \begin{align*}
      A^{-(n+1)}\vec{x} &= A^{-1}(A^{-n}\vec{x}) \\
      &= A^{-1}(\frac{1}{\lambda^n}\vec{x}) \\
      &= (\frac{1}{\lambda^n})(A^{-1}\vec{x}) \\
      &= (\frac{1}{\lambda^n})(\frac{1}{\lambda}\vec{x}) \\
      &= (\frac{1}{\lambda^{n+1}})\vec{x}
    \end{align*}
  \end{enumerate}
  \item Suppose \( A \) is \( n\times n \), and \( A \) has eigenvectors
  \( \vec{v_1},\dots,\vec{v_m} \) with corresponding eigenvalues \( \lambda_1,
  \dots,\lambda_m \). If \( \vec{x}\in\R^n \) is \( \vec{x} = c_1\vec{v_1}+
  \dots+c_m\vec{v_m} \), then for any integer \( k \):
  \[ A^k\vec{x} = c_1\lambda_1^k\vec{v_1}+\dots+c_m\lambda_m^k\vec{v_m} \]
  \textbf{Proof:}
  \begin{align*}
    \vec{x} &= c_1\vec{v_1}+\dots+c_m\vec{v_m} \\
    A^k\vec{x} &= A^k(c_1\vec{v_1}+\dots+c_m\vec{v_m}) \\
    &= c_1A^k\vec{v_1}+\dots+c_mA^k\vec{v_m} \\
    &= c_1\lambda_1^k\vec{v_1}+\dots+c_m\lambda_m^k\vec{v_m}
  \end{align*}
  \item Let \( A \) be an \( n\times n \) matrix. Let \( \lambda_1,\dots,
  \lambda_m \) be distinct eigenvalues of \( A \). Let \( \vec{v_1},\dots,
  \vec{v_m} \) be the corresponding eigenvectors. Then \( \vec{v_1},\dots,
  \vec{v_m} \) are linearly independent. \\
  \textbf{Proof (by contradiction):} \\
  Suppose \( \vec{v_1},\dots,\vec{v_m} \) are linearly dependent. Choose the
  smallest index \( k+1 \) such that \( \overrightarrow{v_{k+1}} =
  c_1\vec{v_1}+\dots+c_k\vec{v_k} \). Note that by the minimality of \( k+1 \),
  \( \vec{v_1}, \dots,\vec{v_k} \) are linearly dependent.
  \begin{align*}
    A\overrightarrow{v_{k+1}} &= A(c_1\vec{v_1}+\dots+c_k\vec{v_k}) \\
    \lambda_{k+1}\overrightarrow{v_{k+1}} &=
      c_1\lambda_1\vec{v_1}+\dots+c_k\lambda_k\vec{v_k} \\
    \lambda_{k+1}\overrightarrow{v_{k+1}} &= c_1\lambda_{k+1}\vec{v_1}+\dots+
      c_k\lambda_{k+1}\vec{v_k} \\
    \vec{0} &= c_1(\lambda_1-\lambda_{k+1})\vec{v_1}+\dots+
      c_k(\lambda_k-\lambda_{k+1})\vec{v_k}
  \end{align*}
  By the linear independence of \( \vec{v_1},\dots,\vec{v_k} \) the scalars
  are zero, this implies that \( c_1 = c_2 = -c_k = 0 \) and thus
  \( \overrightarrow{v_{k+1}} = \vec{0} \). Since eigenvectors cannot be 0,
  this is a contradiction and thus our initial assumption must be false.
\end{enumerate}

\begin{center}
  You can find all my notes at \url{http://omgimanerd.tech/notes}. If you have
  any questions, comments, or concerns, please contact me at
  alvin@omgimanerd.tech
\end{center}

\end{document}

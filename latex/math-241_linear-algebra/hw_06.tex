\documentclass{math}

\usepackage{enumerate}

\geometry{letterpaper, margin=0.5in}

\title{Linear Algebra: Homework 6}
\author{Alvin Lin}
\date{August 2016 - December 2016}

\begin{document}

\maketitle

\section*{Section 3.2}

\subsubsection*{Exercise 1}
Solve the equation for X, given that:
\[ A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} \quad
  B = \begin{bmatrix}-1 & 0 \\ 1 & 1\end{bmatrix} \]
\begin{align*}
  X-2A+3B &= 0 \\
  X &= 2A-3B \\
  &= \begin{bmatrix}2 & 4 \\ 6 & 8\end{bmatrix}-
    \begin{bmatrix}-3 & 0 \\ 3 & 3\end{bmatrix} \\
  &= \begin{bmatrix}5 & 6 \\ 3 & 5\end{bmatrix}
\end{align*}

\subsubsection*{Exercise 3}
Solve the equation for X, given that:
\[ A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} \quad
  B = \begin{bmatrix}-1 & 0 \\ 1 & 1\end{bmatrix} \]
\begin{align*}
  2(A+2B) &= 3X \\
  X &= \frac{2(A+2B)}{3} \\
  &= \frac{2}{3}\left(\begin{bmatrix}
    1 & 2 \\
    3 & 4
  \end{bmatrix}+\begin{bmatrix}
    -2 & 0 \\
    2 & 2
  \end{bmatrix}\right) \\
  &= \begin{bmatrix}
    -\frac{2}{3} & \frac{4}{3} \\[0.5em]
    \frac{10}{3} & \frac{12}{3}
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 5}
Write \( B \) as a linear combination of the other matrices if possible.
\[ B = \begin{bmatrix}2 & 5 \\ 0 & 3\end{bmatrix} \quad
  A_1 = \begin{bmatrix}1 & 2 \\ -1 & 1\end{bmatrix} \quad
  A_2 = \begin{bmatrix}0 & 1 \\ 2 & 1\end{bmatrix} \]
\[ B = 2A_1+A_2 \]

\subsubsection*{Exercise 7}
Write \( B \) as a linear combination of the other matrices if possible.
\[ B = \begin{bmatrix}3 & 1 & 1 \\ 0 & 1 & 0\end{bmatrix} \quad
  A_1 = \begin{bmatrix}1 & 0 & -1 \\ 0 & 1 & 0\end{bmatrix} \quad
  A_2 = \begin{bmatrix}-1 & 2 & 0 \\ 0 & 1 & 0\end{bmatrix} \quad
  A_3 = \begin{bmatrix}1 & 1 & 1 \\ 0 & 0 & 0\end{bmatrix} \]
Not possible.

\subsubsection*{Exercise 9}
Find the general form of the span of the indicated matrices.
\[ A_1 = \begin{bmatrix}1 & 2 \\ -1 & 1\end{bmatrix} \quad
  A_2 = \begin{bmatrix}0 & 1 \\ 2 & 1\end{bmatrix} \]
\begin{align*}
  span(A_1,A_2) &= c_1\begin{bmatrix}1 & 2 \\ -1 & 1\end{bmatrix}+
    c_2\begin{bmatrix}0 & 1 \\ 2 & 1\end{bmatrix} \\
  &= \begin{bmatrix}
    c_1 & 2c_1+c_2 \\
    -c_1+2c_2 & c_1+c_2
  \end{bmatrix} \\
  &= \begin{bmatrix}
    w & x \\
    y & z
  \end{bmatrix} \\
  \left[\begin{array}{cc|c}
    1 & 0 & w \\
    2 & 1 & x \\
    -1 & 2 & y \\
    1 & 1 & z
  \end{array}\right] &= \begin{bmatrix}
    1 & 0 & w \\
    1 & 0 & x-z \\
    0 & 2 & y+w \\
    1 & 1 & z
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & w \\
    1 & 0 & x-z \\
    0 & 1 & \frac{y+w}{2} \\
    0 & 1 & z-w
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & w \\
    0 & 0 & x-z-w \\
    0 & 1 & \frac{y+w}{2} \\
    0 & 0 & z-w-\frac{y+w}{2}
  \end{bmatrix} \\
  w &= x-z \\
  z-w &= \frac{y+w}{2} \\
  2(z-w) &= y+w \\
  2z-3w &= y \\
  2z-3(x-z) &= y \\
  5z-3x &= y \\
  span(A_1,A_2) &= \begin{bmatrix}
    x-z & x \\
    5z-3x & z
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 11}
Find the general form of the span of the indicated matrices.
\[ A_1 = \begin{bmatrix}1 & 0 & -1 \\ 0 & 1 & 0\end{bmatrix} \quad
  A_2 = \begin{bmatrix}-1 & 2 & 0 \\ 0 & 1 & 0\end{bmatrix} \quad
  A_3 = \begin{bmatrix}1 & 1 & 1 \\ 0 & 0 & 0\end{bmatrix} \]
\begin{align*}
  span(A_1,A_2,A_3) &= c_1A_1+c_2A_2+c_3A_3 \\
  &= \begin{bmatrix}
    c_1-c_2+c_3 & 2c_2+c_3 & -c_1+c_3 \\
    0 & c_1+c_2 & 0
  \end{bmatrix} \\
  &= \begin{bmatrix}
    u & v & w \\
    x & y & z
  \end{bmatrix} \\
  \begin{bmatrix}
    1 & -1 & 1 & u \\
    0 & 2 & 1 & v \\
    -1 & 0 & 1 & w \\
    0 & 0 & 0 & x \\
    1 & 1 & 0 & y \\
    0 & 0 & 0 & z
  \end{bmatrix} &= \begin{bmatrix}
    1 & -1 & 1 & u \\
    1 & 1 & 0 & y \\
    0 & 2 & 1 & v \\
    -1 & 0 & 1 & w \\
    0 & 0 & 0 & x \\
    0 & 0 & 0 & z
  \end{bmatrix} \\
  &= \begin{bmatrix}
    0 & -2 & 1 & u-y \\
    0 & 2 & 1 & v \\
    1 & 1 & 0 & y \\
    -1 & 0 & 1 & w \\
    0 & 0 & 0 & x \\
    0 & 0 & 0 & z
  \end{bmatrix} \\
  &= \begin{bmatrix}
    0 & 0 & 2 & u-y+v \\
    0 & 1 & 0 & v-y-w \\
    0 & 2 & 2 & 2y+2w \\
    -1 & 0 & 1 & w \\
    0 & 0 & 0 & x \\
    0 & 0 & 0 & z
  \end{bmatrix} \\
  &= \begin{bmatrix}
    0 & 0 & 2 & u-y+v \\
    0 & 1 & 0 & v-y-w \\
    0 & 0 & 0 & 2y+2w-u+y-v-2(v-y-w) \\
    -1 & 0 & 1 & w \\
    0 & 0 & 0 & x \\
    0 & 0 & 0 & z
  \end{bmatrix} \\
  0 &= 2y+2w-u+y-v-2(v-y-w) \\
  0 &= 5y+4w-u-3v \\
  span(A_1,A_2,A_3) &= \begin{bmatrix}
    5y+4w-3v & \frac{5y+4w-u}{3} & \frac{u+3v-5y}{4} \\
    x & \frac{u+3v-4w}{5} & z
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 13}
Determine whether the given matrices are linearly independent.
\[ \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} \quad
  \begin{bmatrix}4 & 3 \\ 2 & 1\end{bmatrix} \]
\begin{align*}
  \begin{bmatrix}
    c_1+4c_2 & 2c_1+3c_2 \\
    3c_1+2c_2 & 4c_1+c_2
  \end{bmatrix} &= \begin{bmatrix}
    0 & 0 \\
    0 & 0
  \end{bmatrix} \\
  \begin{bmatrix}
    1 & 4 & 0 \\
    2 & 3 & 0 \\
    3 & 2 & 0 \\
    4 & 1 & 0
  \end{bmatrix} &= \begin{bmatrix}
    -5 & 0 & 0 \\
    -10 & 0 & 0 \\
    3 & 2 & 0 \\
    4 & 1 & 0
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
\end{align*}
The only solution for this is \( c_1 = c_2 = 0 \), so the matrices are linearly
independent.

\subsubsection*{Exercise 15}
Determine whether the given matrices are linearly independent.
\[ \begin{bmatrix} 0 & 1 \\ 5 & 2 \\ -1 & 0\end{bmatrix} \quad
  \begin{bmatrix}1 & 0 \\ 2 & 3 \\ 1 & 1\end{bmatrix} \quad
  \begin{bmatrix}-2 & -1 \\ 0 & 1 \\ 0 & 2\end{bmatrix} \quad
  \begin{bmatrix}-1 & -3 \\ 1 & 9 \\ 4 & 5\end{bmatrix} \]
\begin{align*}
  \begin{bmatrix}
    0c_1+1c_2-2c_3-1c_4 & 1c_1+0c_2-1c_3-3c_4 \\
    5c_1+2c_2+0c_3+1c_4 & 2c_1+3c_2+1c_3+9c_4 \\
    -1c_1+1c_2+0c_3+4c_4 & 0c_1+1c_2+2c_3+5c_4
  \end{bmatrix} &= \begin{bmatrix}
    0 & 0 \\
    0 & 0 \\
    0 & 0
  \end{bmatrix} \\
  \begin{bmatrix}
    0 & 1 & -2 & -1 & 0 \\
    1 & 0 & -1 & -3 & 0 \\
    5 & 2 & 0 & 1 & 0 \\
    2 & 3 & 1 & 9 & 0 \\
    -1 & 1 & 0 & 4 & 0 \\
    0 & 1 & 2 & 5 & 0
  \end{bmatrix} &= \begin{bmatrix}
    1 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0
  \end{bmatrix}
\end{align*}
The only solution for this is \( c_1 = c_2 = c_3 = c_4 = 0 \), so the matrices
are linearly independent.

\subsubsection*{Exercise 22}
Prove that, for square matrices \( A \) and \( B \), \( AB = BA \) if and only
if \( (A-B)(A+B) = A^2-B^2 \).
\begin{align*}
  \text{Assume } AB &= BA \\
  AB-BA &= 0 \\
  (A-B)(A+B) &= A^2+AB-BA-B^2 \\
  (A-B)(A+B) &= A^2+0-B^2 \\
  &= A^2-B^2
\end{align*}
Converse:
\begin{align*}
  \text{Assume } (A-B)(A+B) &= A^2-B^2 \\
  A^2-AB-BA-B^2 &= A^2-B^2 \\
  AB-BA &= 0 \\
  AB &= BA
\end{align*}

\subsubsection*{Exercise 29}
Prove that the product of two upper triangular \( n\times n \) matrices is
upper triangular. \\
\begin{align*}
  A &= \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    0 & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & \dots & a_{nn}
  \end{bmatrix} \\
  B &= \begin{bmatrix}
    b_{11} & b_{12} & \dots & b_{1n} \\
    0 & b_{22} & \dots & b_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & \dots & b_{nn}
  \end{bmatrix} \\
  AB &= \begin{bmatrix}
    \sum_{i=1}^{n}a_{1i}b_{i1} & \sum_{i=1}^{n}a_{1i}b_{i2} & \dots &
      \sum_{i=1}^{n}a_{1i}b_{in} \\[1em]
    \sum_{i=1}^{n}a_{2i}b_{i1} & \sum_{i=1}^{n}a_{2i}b_{i2} & \dots &
      \sum_{i=1}^{n}a_{2i}b_{in} \\[1em]
    \vdots & \vdots & \vdots & \vdots \\[1em]
    \sum_{i=1}^{n}a_{ni}b_{i1} & \sum_{i=1}^{n}a_{ni}b_{i2} & \dots &
      \sum_{i=1}^{n}a_{ni}b_{in} \\[1em]
  \end{bmatrix} \\
  &= \begin{bmatrix}
    a_{11}b_{11} & \sum_{i=1}^{n}a_{1i}b_{i2} & \dots &
      \sum_{i=1}^{n}a_{1i}b_{in} \\[1em]
    0 & a_{22}b_{22} & \dots & \sum_{i=1}^{n}a_{2i}b_{in} \\[1em]
    \vdots & \vdots & \vdots & \vdots \\[1em]
    0 & 0 & 0 & a_{nn}b_{nn} \\[1em]
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 30}
Prove Theorem 3.4(a)-(c).
\begin{enumerate}[(a)]
  \item \( (A^T)^T = A \)
  \begin{align*}
    B &= A^T \\
    \therefore b_{ij} &= a_{ji} \\
    C &= B^T = (A^T)^T \\
    \therefore c_{ij} &= b_{ji} \\
    \therefore a_{ij} &= b_{ji} = c_{ij} \\
    A &= C = (A^T)^T
  \end{align*}
  \item \( (A+B)^T = A^T+B^T \)
  \begin{align*}
    (A+B)^T &= C \\
    a_{ij}+b_{ij} &= c_{ij} \\
    A^T+B^T &= C \\
    \therefore A^T+B^T &= (A+B)^T
  \end{align*}
  \item \( (kA)^T = k(A^T) \)
  \begin{align*}
    B &= A^T \\
    \therefore b_{ij} &= a_{ji} \\
    kB &= k(A^T) \\
    kb_{ij} &= k(a_{ji}) \\
    &= (kA)^T \\
    k(B^T) &= k(A^T) = (kA)^T
  \end{align*}
\end{enumerate}

\subsubsection*{Exercise 31}
Prove Theorem 3.4(e).
\[ (A^r)^T = (A^T)^r \text{ for all nonnegative integers } r \]
Base Case (\( r = 1 \)):
\[ (A^1)^T = A^T = (A^T)^1 \]
Induction Hypothesis:
\[ (A^r)^T = (A^T)^r \text{ for } r\ge1 \]
Induction:
\begin{align*}
  (A^{r+1})^T &= (A^rA^1)^T \\
  &= (A^r)^T(A^1)^T \\
  &= (A^T)^r(A^T)^1 \text{ by the base case and induction hypothesis} \\
  &= (A^T)^{r+1}
\end{align*}

\subsubsection*{Exercise 32}
Using induction, prove that for all \( n\ge1 \), \( (A_1+A_2+\dots+A_n)^T =
A_1^T+A_2^T+\dots+A_n^T \). \\
Base Case (\( n = 1 \)):
\[ (A_1)^T = A_1^T \]
Induction Hypothesis:
\[ (A_1+A_2+\dots+A_n)^T = A_1^T+A_2^T+\dots+A_n^T \]
Induction:
\begin{align*}
  (A_1+A_2+\dots+A_n+A_{n+1})^T &= (A_1+A_2+\dots+A_n)^T+A_{n+1}^T
    \text{ by Theorem 3.4b} \\
  &= A_1^T+A_2^T+\dots+A_n^T+A_{n+1}^T \text{ by the induction hypothesis}
\end{align*}

\subsubsection*{Exercise 33}
Using induction, prove that for all \( n\ge1 \), \( (A_1A_2\dots A_n)^T =
A_n^T\dots A_2^TA_1^T \). \\
Base Case (\( n = 1 \)):
\[ (A_1)^T = A_1^T \]
Induction Hypothesis:
\[ (A_1A_2\dots A_n)^T = A_n^T\dots A_2^TA_1^T \]
Induction:
\begin{align*}
  (A_1A_2\dots A_nA_{n+1})^T &= ((A_1A_2\dots A_n)A_{n+1})^T \\
  &= A_{n+1}^T(A_1A_2\dots A_n)^T \text{ by Theorem 3.4d} \\
  &= A_{n+1}^TA_n^T\dots A_2^TA_1^T \text{ by the induction hypothesis}
\end{align*}

\subsubsection*{Exercise 34}
Prove Theorem 3.5(b): For any matrix \( A \), \( AA^T \) and \( A^TA \) are
symmetric matrices.
\begin{align*}
  A &= \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    a_{21} & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{n1} & a_{n2} & \dots & a_{nn}
  \end{bmatrix} \\
  A^T &= \begin{bmatrix}
    a_{11} & a_{21} & \dots & a_{n1} \\
    a_{12} & a_{22} & \dots & a_{n2} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{1n} & a_{2n} & \dots & a_{nn}
  \end{bmatrix} \\
  AA^T &= A^TA = \begin{bmatrix}
    \sum_{i=1}^{n}a_{1i}a_{1i} & \sum_{i=1}^{n}a_{1i}a_{2i} & \dots &
      \sum_{i=1}^{n}a_{1i}a_{ni} \\[1em]
    \sum_{i=1}^{n}a_{2i}a_{1i} & \sum_{i=1}^{n}a_{2i}a_{2i} & \dots &
      \sum_{i=1}^{n}a_{2i}a_{ni} \\[1em]
    \vdots & \vdots & \vdots & \vdots \\[1em]
    \sum_{i=1}^{n}a_{ni}a_{1i} & \sum_{i=1}^{n}a_{ni}a_{2i} & \dots &
      \sum_{i=1}^{n}a_{ni}a_{ni}
  \end{bmatrix} \\
  &= \begin{bmatrix}
    \sum_{i=1}^{n}a_{1i}a_{1i} & \sum_{i=1}^{n}a_{1i}a_{2i} & \dots &
      \sum_{i=1}^{n}a_{1i}a_{ni} \\[1em]
    \sum_{i=1}^{n}a_{1i}a_{2i} & \sum_{i=1}^{n}a_{2i}a_{2i} & \dots &
      \sum_{i=1}^{n}a_{2i}a_{ni} \\[1em]
    \vdots & \vdots & \vdots & \vdots \\[1em]
    \sum_{i=1}^{n}a_{1i}a_{ni} & \sum_{i=1}^{n}a_{2i}a_{ni} & \dots &
      \sum_{i=1}^{n}a_{ni}a_{ni}
  \end{bmatrix} \\
  \sum_{i=1}^{n}a_{1i}a_{i1} &= \sum_{i=1}^{n}a_{i1}a_{1i} \text{ and so on...}
\end{align*}

\subsubsection*{Exercise 37}
Which of the following matrices are skew-symmetric?
\begin{enumerate}[(a)]
  \item \( A = \begin{bmatrix}1 & 2 \\ -2 & 3\end{bmatrix} \) Diagonal does not
  contain zeroes. Not skew-symmetric.
  \item \( A = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix} \quad A^T = -A \).
  This matrix is skew-symmetric.
  \item \( A = \begin{bmatrix}0 & 3 & -1 \\ -3 & 0 & 2 \\ 1 & -2 & 0
  \end{bmatrix} \quad A^T = -A \). This matrix is skew-symmetric.
  \item \( A = \begin{bmatrix}0 & 1 & 2 \\ -1 & 0 & 5 \\ 2 & 5 & 0
  \end{bmatrix} \)
  \begin{align*}
    A^T &= \begin{bmatrix}
      0 & -1 & 2 \\
      1 & 0 & 5 \\
      2 & 5 & 0
    \end{bmatrix} \\
    -A &= \begin{bmatrix}
      0 & -1 & -2 \\
      1 & 0 & -5 \\
      -2 & -5 & 0
    \end{bmatrix} \\
    A^T &\ne -A
  \end{align*}
  This matrix is not skew-symmetric.
\end{enumerate}

\subsubsection*{Exercise 39}
Prove that the main diagonal of a skew-symmetric matrix must consist entirely
of zeros. \\
Let \( A \) be a skew-symmetric matrix.
\begin{align*}
  a_{ij} &= -a{ji} \\
  i &= j \text{ along the diagonal} \\
  a_{ii} &= -a_{ii} \\
  2a_{ii} &= 0 \\
  a_{ii} &= 0
\end{align*}

\subsubsection*{Exercise 40}
Prove that if \( A \) and \( B \) are skew-symmetric \( n\times n \) matrices,
then so is \( A+B \).
\begin{align*}
  a_{ij} &= -a_{ji} \\
  b_{ij} &= -b_{ji} \\
  \text{Let } C &= A+B \\
  c_{ij} &= a_{ij}+b_{ij} \\
  &= -a_{ji}-b_{ji} \\
  &= -(a_{ji}+b_{ji}) \\
  &= -c_{ji}
\end{align*}

\subsubsection*{Exercise 41}
If \( A \) and \( B \) skew-symmetric \( 2\times2 \) matrices, under what
conditions is \( AB \) skew-symmetric?
\begin{align*}
  A &= \begin{bmatrix}
    0 & a \\
    -a & 0
  \end{bmatrix} \\
  B &= \begin{bmatrix}
    0 & b \\
    -b & 0
  \end{bmatrix} \\
  AB &= \begin{bmatrix}
    -ab & 0 \\
    0 & -ab
  \end{bmatrix} \\
  -ab &= 0 \\
  \therefore a &= 0 \text{ or } b = 0
\end{align*}
Either \( A \) or \( B \) must be filled with zeros.

\subsubsection*{Exercise 42}
Prove that if \( A \) is an \( n\times n \) matrix, then \( A-A^T \) is
skew-symmetric.
\begin{align*}
  (A-A^T)^T &= A^T-(A^T)^T \\
  &= A^T-A \\
  &= (-1)(A-A^T)
\end{align*}
Therefore \( A-A^T \) is skew-symmetric.

\subsubsection*{Exercise 43}
\begin{enumerate}[(a)]
  \item Prove that any square matrix \( A \) can be written as the sum of a
  symmetric matrix and a skew-symmetric matrix.
  \begin{align*}
    A &= A+A^T-A^T \\
    &= A+\frac{1}{2}A^T-\frac{1}{2}A^T \\
    &= \frac{1}{2}A+\frac{1}{2}A^T+\frac{1}{2}A-\frac{1}{2}A^T \\
    &= \frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)
  \end{align*}
  \( A+A^T \) and \( A-A^T \) are symmetric and skew-symmetric matrices,
  respectively.
  \item Illustrate part (a) for the matrix \( A = \begin{bmatrix}1 & 2 & 3 \\
  4 & 5 & 6 \\ 7 & 8 & 9\end{bmatrix} \).
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
    \end{bmatrix} &= \frac{1}{2}\left(\begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
    \end{bmatrix}+\begin{bmatrix}
      1 & 4 & 7 \\
      2 & 5 & 8 \\
      3 & 6 & 9 \\
    \end{bmatrix}\right)+\frac{1}{2}\left(\begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
    \end{bmatrix}-\begin{bmatrix}
      1 & 4 & 7 \\
      2 & 5 & 8 \\
      3 & 6 & 9 \\
    \end{bmatrix}\right)
  \end{align*}
\end{enumerate}

\subsubsection*{Exercise 44}
If \( A \) and \( B \) are \( n\times n \) matrices, prove the following
properties of the trace:
\begin{enumerate}[(a)]
  \item \( tr(A+B) = tr(A)+tr(B) \)
  \begin{align*}
    tr(A+B) &= \sum_{i=1}^{n}(a_{ii}+b_{ii}) \\
    &= \sum_{i=1}^{n}a_{ii}+\sum_{i=1}^{n}b_{ii} \\
    &= tr(A)+tr(B)
  \end{align*}
  \item \( tr(kA) = k~tr(A) \)
  \begin{align*}
    tr(kA) &= \sum_{i=1}^{n}(ka_{ii}) \\
    &= k\left(\sum_{i=1}^{n}a_{ii}\right) \\
    &= k~tr(A)
  \end{align*}
\end{enumerate}

\subsubsection*{Exercise 45}
Prove that if \( A \) and \( B \) are \( n\times n \) matrices, then \( tr(AB)
= tr(BA) \).
\begin{align*}
  tr(AB) &= \sum_{i=1}^{n}row_i(A)\cdot col_i(B) \\
  &= \sum_{i=1}^{n}\left(\sum_{j=1}^{n}a_{ij}b_{ji}\right) \\
  &= \sum_{j=1}^{n}\left(\sum_{i=1}^{n}b_{ji}a_{ij}\right) \\
  &= tr(BA)
\end{align*}

\subsubsection*{Exercise 46}
If \( A \) is any matrix, to what is \( tr(AA^T) \) equal?
\begin{align*}
  C &= AA^T \\
  c_{ij} &= \sum_{i=1}^{n}row_i(A)\cdot col_i(A^T) \\
  &= \sum_{i=1}^{n}row_i(A)\cdot row_i(A) \\
  tr(AA^T) &= \sum_{i=1}^{n}\|row_i(A)\|^2
\end{align*}

\subsubsection*{Exercise 47}
Show that there are no \( 2\times2 \) matrices \( A \) and \( B \) such that
\( AB-BA = I_2 \).
\begin{align*}
  tr(AB-BA) &= tr(AB+(-1)BA) \\
  &= tr(AB)+tr((-1)BA) \\
  &= tr(AB)-tr(BA) \\
  &= 0 \\
  \therefore AB-BA &= \begin{bmatrix}
    0 & a_{11}b_{12}+a_{12}b_{22}-b_{11}a_{12}+b_{12}a_{22} \\
    a_{21}b_{11}+a_{22}b_{21}-b_{21}a_{11}+b_{22}a_{21} & 0
  \end{bmatrix} \\
  &\ne I_2
\end{align*}

\section*{Section 3.3}

\subsubsection*{Exercise 1}
Find the inverse of the given matrix (if it exists) using Theorem 3.8.
\[ A = \begin{bmatrix}4 & 7 \\ 1 & 2\end{bmatrix} \]
\begin{align*}
  ad-bc &= 1 \\
  A' &= \begin{bmatrix}
    2 & -7 \\
    -1 & 4
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 3}
Find the inverse of the given matrix (if it exists) using Theorem 3.8.
\[ A = \begin{bmatrix}3 & 4 \\ 6 & 8\end{bmatrix} \]
\[ ad-bc = 0 \]
The matrix is not invertible.

\subsubsection*{Exercise 5}
Find the inverse of the given matrix (if it exists) using Theorem 3.8.
\[ A = \begin{bmatrix}\frac{3}{4} & \frac{3}{5} \\[0.25em]
  \frac{5}{6} & \frac{2}{3}\end{bmatrix} \]
\begin{align*}
  ad-bc &= \frac{1}{2}-\frac{1}{2} \\
  &= 0
\end{align*}
The matrix is not invertible.

\subsubsection*{Exercise 7}
Find the inverse of the given matrix (if it exists) using Theorem 3.8.
\[ A = \begin{bmatrix}-1.5 & -4.2 \\[0.25em] 0.5 & 2.4\end{bmatrix} \]
\begin{align*}
  ad-bc &= -3.6-(-2.1) \\
  &= -1.5 \\
  A' &= \begin{bmatrix}
    \frac{2.4}{-1.5} & -\frac{-4.2}{-1.5} \\[0.25em]
    -\frac{0.5}{-1.5} & \frac{-1.5}{-1.5}
  \end{bmatrix} \\
  &= \begin{bmatrix}
    -1.6 & -2.8 \\
    0.33 & 1
  \end{bmatrix} \\
\end{align*}

\subsubsection*{Exercise 9}
Find the inverse of the given matrix (if it exists) using Theorem 3.8.
\[ A = \begin{bmatrix}a & -b \\ b & a\end{bmatrix} \]
\begin{align*}
  \frac{1}{\det(A)} &= \frac{1}{a^2+b^2} \\
  A' &= \begin{bmatrix}
    \frac{a}{a^2+b^2} & \frac{b}{a^2+b^2} \\[0.25em]
    -\frac{b}{a^2+b^2} & \frac{a}{a^2+b^2}
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 11}
Solve the given system using the method of Example 3.25.
\begin{align*}
  2x+y &= -1 \\
  5x+3y &= 2 \\
  \begin{bmatrix}x \\ y\end{bmatrix} &= \left(\begin{bmatrix}
    2 & 1 \\
    5 & 3
  \end{bmatrix}\right)^{-1}\begin{bmatrix}-1 \\ 2\end{bmatrix} \\
  &= \frac{1}{6-5}\begin{bmatrix}
    3 & -1 \\
    -5 & 2
  \end{bmatrix}\begin{bmatrix}-1 \\ 2\end{bmatrix} \\
  &= \begin{bmatrix}-5 \\ 9\end{bmatrix}
\end{align*}

\subsubsection*{Exercise 18}
By induction, prove that if \( A_1,A_2,\dots,A_n \) are invertible matrices of
the same size, then the product \( A_1A_2\dots A_n \) is invertible and
\( (A_1A_2\dots A_n)^{-1} = A_n^{-1}\dots A_2^{-1}A_1^{-1} \). \\
Base Case:
\[ (A_1A_2)^{-1} = A_2^{-1}A_1^{-1} \]
Induction Hypothesis:
\[ (A_1A_2\dots A_n)^{-1} = A_n^{-1}\dots A_2^{-1}A_1^{-1} \]
Induction:
\begin{align*}
  (A_1A_2\dots A_nA_{n+1})^{-1} &= ((A_1A_2\dots A_n)(A_{n+1}))^{-1} \\
  &= A_{n+1}^{-1}(A_1A_2\dots A_n)^{-1} \text{ by Theorem 3.9c} \\
  &= A_{n+1}^{-1}A_n^{-1}\dots A_2^{-1}A_1^{-1}
    \text{ by the induction hypothesis}
\end{align*}

\subsubsection*{Exercise 44}
A square matrix \( A \) is called \textbf{idempotent} if \( A^2 = A \).
\begin{enumerate}[(a)]
  \item Find three idempotent \( 2\times2 \) matrices.
  \[ \begin{bmatrix}0 & 0 \\ 0 & 0\end{bmatrix} \quad
    \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} \quad
    \begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix} \]
  \item Prove that the only invertible idempotent \( n\times n \) matrix is the
  identity matrix.
  \begin{align*}
    A &= A^2 \\
    A(A^{-1}) &= A^2(A^{-1}) \\
    I_n &= AAA^{-1} \\
    I_n &= AI_n \\
    A &= I_n
  \end{align*}
\end{enumerate}

\subsubsection*{Exercise 45}
Show that if \( A \) is a a square matrix that satisfies the equation
\( A^2-2A+I = 0 \), then \( A^{-1} = 2I-A \).
\begin{align*}
  A^2-2A+I &= 0 \\
  A^2(A^{-1})-2A(A^{-1})+I(A^{-1}) &= 0(A^{-1}) \\
  AI-2(I)+A^{-1} &= 0 \\
  A-2I+A^{-1} &= 0 \\
  A^{-1} &= 2I-A
\end{align*}

\subsubsection*{Exercise 46}
Prove that if a symmetric matris is invertible, then its inverse is symmetric
also.
\begin{align*}
  A &= A^T \\
  (A^{-1})^T &= (A^T)^{-1} \\
  (A^{-1})^T &= A^{-1}
\end{align*}
Since the transpose of the inverse is equal to the inverse, the inverse must be
symmetric.

\subsubsection*{Exercise 47}
Prove that if \( A \) and \( B \) are square matrices and \( AB \) is
invertible, then both \( A \) and \( B \) are invertible. \\
\begin{align*}
  B\vec{x} &= 0 \\
  (A)B\vec{x} &= (A)0 \\
  AB\vec{x} &= 0 \\
  \vec{x} &= 0 \quad \text{Theorem 3.12c} \\
  & \therefore B \text{ is invertible} \\
  A &= AI \\
  &= A(BB^{-1}) \\
  &= (AB)(B^{-1})
\end{align*}
Since \( A \) is the product of two invertible matrices, \( A \) is invertible
as well.

\subsubsection*{Exercise 49}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    -2 & 4 & 1 & 0 \\
    3 & -1 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    10 & 0 & 1 & 4 \\
    3 & -1 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{10} & \frac{2}{5} \\[0.25em]
    -3 & 1 & 0 & -1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{10} & \frac{2}{5} \\[0.25em]
    0 & 1 & \frac{3}{10} & \frac{1}{5}
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 51}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    1 & a & 1 & 0 \\
    -a & 1 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    1 & a & 1 & 0 \\
    a^2 & -a & 0 & -a
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1+a^2 & 0 & 1 & -a \\
    a^2 & -a & 0 & -a
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{1+a^2} & \frac{-a}{1+a^2} \\[0.25em]
    1 & \frac{-1}{a} & 0 & \frac{-1}{a}
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{1+a^2} & \frac{-a}{1+a^2} \\[0.25em]
    0 & \frac{-1}{a} & -\frac{1}{1+a^2} & \frac{-1}{a}+\frac{a}{1+a^2}
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{1+a^2} & \frac{-a}{1+a^2} \\[0.25em]
    0 & 1 & \frac{a}{1+a^2} & 1+\frac{-a^2}{1+a^2}
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & \frac{1}{1+a^2} & \frac{-a}{1+a^2} \\[0.25em]
    0 & 1 & \frac{a}{1+a^2} & \frac{1}{1+a^2}
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 53}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    1 & -1 & 2 & 1 & 0 & 0 \\
    3 & 1 & 2 & 0 & 1 & 0 \\
    2 & 3 & -1 & 0 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    1 & -1 & 2 & 1 & 0 & 0 \\
    0 & 4 & -4 & -4 & 1 & 0 \\
    0 & 5 & -5 & -2 & 0 & 1
  \end{bmatrix}
\end{align*}
No inverse exists since the last two rows can cancel each other out.

\subsubsection*{Exercise 55}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    a & 0 & 0 & 1 & 0 & 0 \\
    1 & a & 0 & 0 & 1 & 0 \\
    0 & 1 & a & 0 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    1 & 0 & 0 & \frac{1}{a} & 0 & 0 \\
    1 & a & 0 & 0 & 1 & 0 \\
    0 & 1 & a & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & \frac{1}{a} & 0 & 0 \\
    0 & a & 0 & -\frac{1}{a} & 1 & 0 \\
    0 & 1 & a & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & \frac{1}{a} & 0 & 0 \\
    0 & 1 & 0 & -\frac{1}{a^2} & \frac{1}{a} & 0 \\
    0 & 1 & a & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & \frac{1}{a} & 0 & 0 \\
    0 & 1 & 0 & -\frac{1}{a^2} & \frac{1}{a} & 0 \\
    0 & 0 & a & \frac{1}{a^2} & -\frac{1}{a} & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & \frac{1}{a} & 0 & 0 \\
    0 & 1 & 0 & -\frac{1}{a^2} & \frac{1}{a} & 0 \\
    0 & 0 & 1 & \frac{1}{a^3} & -\frac{1}{a^2} & \frac{1}{a}
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 57}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    0 & -1 & 1 & 0 & 1 & 0 & 0 & 0 \\
    2 & 1 & 0 & 2 & 0 & 1 & 0 & 0 \\
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & 1 & -1 & 0 & 0 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    1 & -1 & 3 &  0 & 0 & 0 & 1 & 0 \\
    2 &  1 & 0 &  2 & 0 & 1 & 0 & 0 \\
    0 & -1 & 1 &  0 & 1 & 0 & 0 & 0 \\
    0 &  1 & 1 & -1 & 0 & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 3 & -6 & 2 & 0 & 1 & -2 & 0 \\
    0 & -1 & 1 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 1 & -1 & 0 & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 3 & -6 & 2 & 0 & 1 & -2 & 0 \\
    0 & -1 & 1 & 0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 2 & -1 & 1 & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & -3 & 2 & 3 & 1 & -2 & 0 \\
    0 & 0 & 2 & -1 & 1 & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & 0 & -1 & -9 & -2 & 4 & -3 \\
    0 & 0 & 1 & -1 & -4 & -1 & 2 & -1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & 1 & 0 & 5 & 1 & -2 & 2 \\
    0 & 0 & 0 & 1 & 9 & 2 & -4 & 3
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & -1 & 3 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & 1 & 0 & 5 & 1 & -2 & 2 \\
    0 & 0 & 0 & 1 & 9 & 2 & -4 & 3
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & 1 & -2 & 0 & 1 & -1 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & 1 & 0 & 5 & 1 & -2 & 2 \\
    0 & 0 & 0 & 1 & 9 & 2 & -4 & 3
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & 1 & -2 & 0 & 1 & -1 \\
    0 & 1 & -4 & 2 & 2 & 1 & -2 & 0 \\
    0 & 0 & 1 & 0 & 5 & 1 & -2 & 2 \\
    0 & 0 & 0 & 1 & 9 & 2 & -4 & 3
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & 0 & -11 & -2 & 5 & -4 \\
    0 & 1 & 0 & 0 & 4 & 1 & -2 & 2 \\
    0 & 0 & 1 & 0 & 5 & 1 & -2 & 2 \\
    0 & 0 & 0 & 1 & 9 & 2 & -4 & 3
  \end{bmatrix}
\end{align*}

\subsubsection*{Exercise 59}
Use the Gauss-Jordan method to find the inverse of the given matrix (if it
exists).
\begin{align*}
  \begin{bmatrix}
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
    a & b & c & d & 0 & 0 & 0 & 1
  \end{bmatrix} &= \begin{bmatrix}
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
    0 & b & c & d & -a & 0 & 0 & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & d & -a & -b & -c & 1
  \end{bmatrix} \\
  &= \begin{bmatrix}
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 & -\frac{a}{d} & -\frac{b}{d} & -\frac{c}{d} & \frac{1}{d}
  \end{bmatrix} \\
\end{align*}

\begin{center}
  If you have any questions, comments, or concerns, please contact me at
  alvin@omgimanerd.tech
\end{center}

\end{document}
